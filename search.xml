<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CRF</title>
    <url>/2021/03/25/CRF/</url>
    <content><![CDATA[<h4 id="1- 马尔科夫随机场"><a href="#1- 马尔科夫随机场" class="headerlink" title="1. 马尔科夫随机场"></a>1. 马尔科夫随机场</h4><ul>
<li><p>场: 在空间的某个区域内, 如果除了个别点以外, 其他的该区域内的每个点 p 都有一个确定的量 $f(p)$, 那么 <strong> 该区域 </strong> 称为 $f(p)$ 的场。</p>
</li>
<li><p>随机过程: 设 $T$ 是一无限实数集, 把依赖于 $t \in T$ 的一族 (无限多个) 随机变量称为随机过程。即为</p>
<script type="math/tex; mode=display">{X(t), t \in T}</script></li>
<li><p>随机场: 如果 $T$ 是 $n$ 维度空间的一个子集, 即 $t$ 是一个 $n$ 维向量, 此时随机过程称为随机场。</p>
</li>
</ul>
<h4 id="2- 条件随机场 CRF-conditional-random-field"><a href="#2- 条件随机场 CRF-conditional-random-field" class="headerlink" title="2. 条件随机场 CRF (conditional random field)"></a>2. 条件随机场 CRF (conditional random field)</h4><p>定义: 设 $X=(x_1, x_2, …, x_n)$ 和 $Y=(y_1, y_2, …, y_n)$ 均为线性表示的随机变量序列。若在 <strong> 给定 </strong> 随机变量序列<strong>x 的条件下</strong>。 随机变量序列 $Y$ 的条件概率分布 $P(Y|X)$ 构成条件随机场, 并满足马尔科夫性:</p>
<script type="math/tex; mode=display">P(Y_i|X, Y1,...,Y_{i-1},Y_{i+1},...,Y_n) = P(Y_i|X,Y_{i-1},Y_{i+1})</script><p>此时称 $P(Y|X)$ 为线性条件随机场。<br><span id="more"></span></p>
<h4 id="3- 线性链条件随机场公式"><a href="#3- 线性链条件随机场公式" class="headerlink" title="3. 线性链条件随机场公式"></a>3. 线性链条件随机场公式</h4><p><img src="https://pic1.zhimg.com/80/v2-ee91ebeecd409eb4a33af209fb8b0f18_720w.jpg" style="width:80%;height:80%;display:inline-block;text-align:center"></p>
<p>特征函数定义:</p>
<p><img src="https://pic1.zhimg.com/80/v2-799f58be49406ee7478ed52d94a3f188_720w.jpg" style="width:80%;height:80%;display:inline-block;text-align:center"></p>
<p>为了简单起见，将转移特征和状态特征及其权值用统一符号表示。条件随机场简化公式如下:</p>
<p><img src="https://pic4.zhimg.com/80/v2-e0f24d7d3c60c3fb0cfba7f10e3f33b3_720w.jpg" style="width:80%;height:80%;display:inline-block;text-align:center"></p>
<h4 id="4- 举例说明"><a href="#4- 举例说明" class="headerlink" title="4. 举例说明"></a>4. 举例说明</h4><p><img src="https://pic3.zhimg.com/80/v2-98c85098271f347be65745d6d5b4e8de_720w.jpg" style="width:80%;height:80%;display:inline-block;text-align:center"></p>
<p><img src="https://pic2.zhimg.com/80/v2-980fd0c5d7c701e69aa3435f51901cb9_720w.jpg" style="width:80%;height:80%;display:inline-block;text-align:center"></p>
<p>则标注序列为 {B、I、I} 的联合概率分布如下：<br><img src="https://pic4.zhimg.com/80/v2-15120e3c0ae1e8c49defca66ac945143_720w.jpg" style="width:80%;height:80%;display:inline-block;text-align:center"></p>
<h4 id="5- 朴素贝叶斯 -HMM- 逻辑回归 -CRF 对比表格"><a href="#5- 朴素贝叶斯 -HMM- 逻辑回归 -CRF 对比表格" class="headerlink" title="5. 朴素贝叶斯, HMM, 逻辑回归, CRF 对比表格"></a>5. 朴素贝叶斯, HMM, 逻辑回归, CRF 对比表格</h4><p><img src="https://pic4.zhimg.com/v2-104861c06cb27665b8d4287d7ab55f9b_r.jpg" style="width:80%;height:80%;display:inline-block;text-align:center"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>原理</th>
<th>特点</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>朴素贝叶斯</td>
<td>贝叶斯公式</td>
<td>条件概率</td>
<td>条件推理</td>
<td>训练复杂</td>
</tr>
<tr>
<td>HMM</td>
<td>马尔科夫性</td>
<td>依赖前一个状态</td>
<td>训练快</td>
<td>局部最优</td>
</tr>
<tr>
<td>逻辑回归</td>
<td>逻辑函数</td>
<td>取值[0,1]</td>
<td>模拟概率</td>
<td>准确率不高</td>
</tr>
<tr>
<td>CRF</td>
<td>Hammersley-Clifford 定理</td>
<td>最大团乘积</td>
<td>特征灵活, 全局最优</td>
<td>设置特征模板</td>
</tr>
</tbody>
</table>
</div>
<h4 id="6-CRF 的发展简史"><a href="#6-CRF 的发展简史" class="headerlink" title="6. CRF 的发展简史"></a>6. CRF 的发展简史</h4><p>CRF 一直时标注问题的基本模型。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>历史</th>
<th>模型</th>
</tr>
</thead>
<tbody>
<tr>
<td>机器学习阶段</td>
<td>CRF</td>
</tr>
<tr>
<td>深度学阶段</td>
<td>BiLSTM-CRF, BILSTM-CNN-CRF</td>
</tr>
<tr>
<td>Attention 阶段</td>
<td>Transformer-CRF, BERT-BiLSTM-CRF</td>
</tr>
</tbody>
</table>
</div>
<h4 id="7-CRF 的应用"><a href="#7-CRF 的应用" class="headerlink" title="7. CRF 的应用"></a>7. CRF 的应用 </h4><h5 id="7-1- 中文分词"><a href="#7-1- 中文分词" class="headerlink" title="7.1. 中文分词"></a>7.1. 中文分词</h5><p> 基于 CRF 由字构词方法的基本思想，基本原理如下：</p>
<p><img src="https://pic2.zhimg.com/80/v2-31b9b12996bef5bfc857eabcc276e899_720w.jpg" style="width:80%;height:80%;display:inline-block;text-align:center"></p>
<p>CRF 中文分词的图结构如下：</p>
<p><img src="https://pic2.zhimg.com/80/v2-8c7825e6a2a700faf96c7ed2aa1c7ed9_720w.jpg" style="width:80%;height:80%;display:inline-block;text-align:center"></p>
<h5 id="7-2- 命名实体识别"><a href="#7-2- 命名实体识别" class="headerlink" title="7.2. 命名实体识别"></a>7.2. 命名实体识别 </h5><p> 基于 CRF 的命名实体识别过程如下：</p>
<p><img src="https://pic1.zhimg.com/80/v2-e6b99a5894e1d65a0b8802ed718b861c_720w.jpg" style="width:80%;height:80%;display:inline-block;text-align:center"></p>
<p>CRF 命名实体识别的图结构如下：</p>
<p><img src="https://pic2.zhimg.com/80/v2-cdacd9ff36747052097e5516360a0155_720w.jpg" style="width:80%;height:80%;display:inline-block;text-align:center"></p>
<h5 id="7-3- 词性标注"><a href="#7-3- 词性标注" class="headerlink" title="7.3. 词性标注"></a>7.3. 词性标注 </h5><p> 基于 CRF 词性标注方法的基本思想，基本原理如下：</p>
<p><img src="https://pic3.zhimg.com/80/v2-d6156399374fb6f9f813301559cb3f32_720w.jpg" style="width:80%;height:80%;display:inline-block;text-align:center"></p>
<p>CRF 中文词性标注的图结构如下：</p>
<p><img src="https://pic2.zhimg.com/80/v2-9d7163ea8fbb8484d1521936fe2ef139_720w.jpg" style="width:80%;height:80%;display:inline-block;text-align:center"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>概率图</tag>
      </tags>
  </entry>
  <entry>
    <title>GBDT, Xgboost 和 LightGBM</title>
    <url>/2021/04/08/GBDT-Xgboost-%E5%92%8C-LightGBM/</url>
    <content><![CDATA[<h4 id="1-Xgboost"><a href="#1-Xgboost" class="headerlink" title="1. Xgboost"></a>1. Xgboost</h4><ul>
<li><p> 目标函数的意义 $Obj(\theta) = L(\theta) + G(\theta)$</p>
<ul>
<li><p>$L(\theta)$ 衡量了模型对于训练数据的拟合能力: 均方根 (回归), 逻辑损失 (分类)。</p>
</li>
<li><p>$G(\theta)$ 测量了模型的复杂度：L1, L2 正则化。</p>
</li>
</ul>
</li>
<li><p> 优化目标函数的意义：</p>
<ul>
<li><ol>
<li> 优化 training loss 鼓励模型去学习一个有用的模型 </li>
</ol>
</li>
<li><ol start="2">
<li> 优化 regularization loss 鼓励去学习一个简单的模型。</li>
</ol>
</li>
</ul>
</li>
<li><p> 对应树结构：$L(\theta)$ 就是各个树的结构，而 $G(\theta)$ 就是树的复杂性 (例如树的节点个数，树的深度等)</p>
</li>
<li><p> 目标函数: $\sum_{i=1}^{n}l(y_i, \hat{y_i}) + \sum_{k}G(f_k), f_k \in \ F$</p>
<ul>
<li><p> 面临的问题： 我们不能使用 SGD 去寻找 $f_k$, 因为他们都是树结构，而不是数值向量。</p>
</li>
<li><p> 解决方法： 加法训练（Addtive Training or called Boosting）<br>$$ \hat{y}_i^{(0)}  = 0 $$<br>$$ \hat{y}_i^{(1)} = f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i) $$<br>$$ \hat{y}_i^{(2)} = f_1(x_i) + f_2(x_i) = \hat{y}_i^{(1)}+f_2(x_i) $$<br>$$ \hat{y}<em>i^{(t)} = \sum</em>{k=1}^tf_k(x_i) = \hat{y}_i^{(t-1)} + f_t(x_i) $$</p>
</li>
<li><p> 如何决定在第 t 轮加入的 $f_t$<br> 第 t 轮的预测 <br>$$ \hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + f_t(x_i)</p>
</li>
</ul>
<p> $$</p>
</li>
</ul>
<span id="more"></span>
<h4 id="GBDT- 与 -XGBoost- 的对比"><a href="#GBDT- 与 -XGBoost- 的对比" class="headerlink" title="GBDT 与 XGBoost 的对比"></a>GBDT 与 XGBoost 的对比 </h4><ul>
<li><p>GBDT 是机器学习算法，XGBoost 是该算法的工程实现 </p>
</li>
<li><p> 在使用 CART 作为及分类器时，XGBoost 显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，提高泛化能力 </p>
</li>
<li><p>GBDT 在模型训练的时候仅使用了代价函数的一阶导数信息，XGBoost 对代价函数进行了二阶泰勒展开，使用了一阶和二阶倒数 </p>
</li>
<li><p>GBDT 采用 CART 作为基分类器，XGBoost 支持多种类型的基分类器，比如线性分类器。</p>
</li>
<li><p>GBDT 在每轮迭代时是使用了全部的数据， XGBoost 则采用了随机森林相似的策略，最数据进行采样，支持列采样，降低过拟合，减少计算。</p>
</li>
<li><p>GBDT 没有设计对缺失值进行处理，XGBoost 可以自动学习出它的分类方法， XGBoost 对缺失值预先学习一个默认的分裂方向 </p>
</li>
<li><p>Shrinkage （减弱），相当于学习速率（Xgboost 中的 eta）, xgboost 每次迭代后，将叶子节点权重乘以该系数，减弱每棵树的影响，让后面有更大的学习空间。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>HMM</title>
    <url>/2021/03/25/HMM/</url>
    <content><![CDATA[<h4 id="1- 马尔可夫性"><a href="#1- 马尔可夫性" class="headerlink" title="1. 马尔可夫性"></a>1. 马尔可夫性 </h4><p> 如果某一时刻 $t \geq 1$ 的随机变量 $x<em>t$ 与其一时刻随机变量 $x</em>{t-1}$ 之间有条件分布 $P({x<em>t}|{x</em>{t-1}})$, 如果 $x<em>t$ 与 $t-1$ 时刻之前的状态都没有关系, 仅与 $x</em>{t-1}$ 状态有关, 这一性质成为马尔科夫性质。</p>
<script type="math/tex; mode=display">P(X_t|X_1, X_2,...,X_t-1) = P(X_t|X_{t-1})</script><h4 id="2- 马尔科夫过程 -or- 马尔科夫链"><a href="#2- 马尔科夫过程 -or- 马尔科夫链" class="headerlink" title="2. 马尔科夫过程 or 马尔科夫链"></a>2. 马尔科夫过程 or 马尔科夫链 </h4><p> 具有马尔科夫性质的序列被成为马尔科夫链条。</p>
<script type="math/tex; mode=display">X = (X_1, X_2, ..., X_t)</script><h4 id="3- 马尔科夫链中状态的计算"><a href="#3- 马尔科夫链中状态的计算" class="headerlink" title="3. 马尔科夫链中状态的计算"></a>3. 马尔科夫链中状态的计算 </h4><p> 马尔科夫过程中的 <strong> 状态 $t$</strong> 的状态分布概率的计算，它可以由前一时刻 $t-1$ 的状态分布和状态转移矩阵算出来。</p>
<script type="math/tex; mode=display">\pi(t) = P\pi(t-1) = P(P\pi(t-2)) = P^2\pi(t-2)</script><p>即 <script type="math/tex">\pi(t) = P^t\pi(0)</script></p>
<span id="more"></span>
<h4 id="4- 隐马尔科夫模型"><a href="#4- 隐马尔科夫模型" class="headerlink" title="4. 隐马尔科夫模型"></a>4. 隐马尔科夫模型</h4><h5 id="4-1- 假设"><a href="#4-1- 假设" class="headerlink" title="4.1 假设"></a>4.1 假设</h5><ul>
<li>观测独立假设: <strong>观测 </strong> 只依赖于该时刻的 <strong> 马尔科夫链的状态</strong>, 与其他观测无关。</li>
<li>马尔科夫性质: 当前状态只与前一状态有关。</li>
</ul>
<h5 id="4-2- 组成"><a href="#4-2- 组成" class="headerlink" title="4.2 组成"></a>4.2 组成</h5><ul>
<li>状态转移矩阵: $A=[a<em>{ij}]</em>{N \times N}$, $a<em>{ij} = P(y_t=s_j|y</em>{t-1}=s_i)$。表示 $t-1$ 时刻处于状态 $s_i$ 的时候，在 $t$ 时刻处于 $s_j$ 的概率。</li>
<li>观测概率矩阵: $B=[b<em>{ij}]</em>{N \times M}$, $b_{ij} = P(x_t=o_j|y_t=s_i)$。表示在 $t$ 时刻处于状态 $s_i$, 生成观测值 $o_j$ 的概率。</li>
<li>初始状态概率: $\pi=(\pi_1, \pi_2, …, \pi_N)$, $\pi_i=P(y_1=s_i)$。表示初始状态为 $s_i$ 的概率。</li>
</ul>
<h5 id="4-3- 联合概率分布"><a href="#4-3- 联合概率分布" class="headerlink" title="4.3 联合概率分布"></a>4.3 联合概率分布</h5><p>$P(o<em>1,s_1,…,o_t,s_t) = P(s_1)P(o_1|s_1)\prod</em>{i=2}^{t}P(s<em>i|s</em>{i-1})P(o_i|s_i)$</p>
<h5 id="4-4- 定义"><a href="#4-4- 定义" class="headerlink" title="4.4 定义"></a>4.4 定义 </h5><p> 通过状态集合 $Q$, 观测集合 $V$, 观测概率矩阵 $B$, 状态转移矩阵 $A$, 初始状态概率向量 $\pi$, 确定一个隐马尔科夫模型，即为:</p>
<script type="math/tex; mode=display">\lambda = (A, B, \pi)</script><h5 id="4-5- 三个基本问题"><a href="#4-5- 三个基本问题" class="headerlink" title="4.5 三个基本问题"></a>4.5 三个基本问题</h5><ul>
<li>识别问题(概率计算算法, 也成为前向算法): 给定模型 $\lambda = (A, B, \pi)$ -&gt; 计算模型与观测序列的匹配程度 $P(O|\lambda)$<ul>
<li>前向算法定义: 输入($\lambda, O$), 输出序列概率 $P(O|\lambda)$</li>
</ul>
</li>
<li>学习问题 (鲍勃 - 韦尔奇算法): 给定观测序列 $O(o_1, o_2, …, o_n)$, 如何训练一个模型 $\lambda = (A, B, \pi)$, 能够使 $P(O|\lambda)$ 最大。</li>
<li>预测算法(维特比算法, 动态规划算法): 给定 $\lambda = (A, B, \pi)$ 和观测序列 $O(o_1, o_2, …, o_n)$, 如果找到此观测序列最匹配的状态序列 $S = (s_1, s_2, …, s_n)$。 由观测样本得到隐状态。</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>问题</th>
<th>解决方法</th>
<th>本质</th>
</tr>
</thead>
<tbody>
<tr>
<td>识别问题</td>
<td>前向算法(Forward Algorithm)</td>
<td>概率的递推计算</td>
</tr>
<tr>
<td>学习问题</td>
<td>Baum-Welch Algorithm</td>
<td>极大似然估计</td>
</tr>
<tr>
<td>解码问题</td>
<td>Viterbi Algorithm</td>
<td>动态规划求最优路径</td>
</tr>
</tbody>
</table>
</div>
<h5 id="4-6- 局限性"><a href="#4-6- 局限性" class="headerlink" title="4.6 局限性"></a>4.6 局限性</h5><ul>
<li>状态值存在 <strong> 长距离的依赖</strong></li>
<li>观测值有非独立的 <strong> 交叉特征</strong></li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>概率图</tag>
      </tags>
  </entry>
  <entry>
    <title>Java 基础 - 集合</title>
    <url>/2021/03/23/java%E5%9F%BA%E7%A1%80-%E9%9B%86%E5%90%88/</url>
    <content><![CDATA[<h3 id="Stack- 栈，-FILO"><a href="#Stack- 栈，-FILO" class="headerlink" title="Stack (栈， FILO)"></a>Stack (栈， FILO)</h3><hr>
<ul>
<li>方法  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Stack&lt;Integer&gt; stk &#x3D; new Stack&lt;&gt;();</span><br><span class="line">(1) 栈顶添加元素: stk.push()</span><br><span class="line">(2) 取出栈顶元素(不删除)： stk.peek()</span><br><span class="line">(3) 取出栈顶元素(并删除)： stk.poll()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<span id="more"></span>
<h3 id="List- 有序列表的集合"><a href="#List- 有序列表的集合" class="headerlink" title="List (有序列表的集合)"></a>List (有序列表的集合)</h3><hr>
<ul>
<li>子类 ArrayList, LinkedList</li>
<li>方法  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1) 添加一个元素： boolean add(val)</span><br><span class="line">(2) 指点索引添加元素： boolean add(idx, val)</span><br><span class="line">(3) 删除某个元素： int remove(val)</span><br><span class="line">(4) 删除指定索引元素：int remove(idx)</span><br><span class="line">(5) 得到列表元素大小：int size()</span><br><span class="line">(6) 得到指定索引元素：int get(idx)</span><br><span class="line">(7) 数组转 list: Integer[] arr &#x3D; &#123;1,2,3&#125;</span><br><span class="line">              List&lt;Integer&gt; list &#x3D; List.of(array)</span><br><span class="line">(8) list 转数组：Object[] arr &#x3D; list.toArray()</span><br><span class="line">              Integer[] arr &#x3D; list.toArray(new Integer[n])</span><br><span class="line">(9) 静态初始化：List&lt;String&gt; list &#x3D; List.of(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)</span><br><span class="line">(10) 判断是否存在某个元素，通过 equals 判断：list.contains(val)  boolean</span><br><span class="line">(11) 返回某个元素的索引： list.indexOf(val) 如果某有返回 -1</span><br><span class="line">(12) 涉及到自定义类 equals 重写：</span><br><span class="line">        public boolean euqals(Object o)&#123;</span><br><span class="line">            if(o isinstanceof E)&#123;</span><br><span class="line">                 Person p &#x3D; (Person) o; &#x2F;&#x2F;Person 为自定义类，包含 name(引用字段), age(非引用字段)</span><br><span class="line">                &#x2F;&#x2F; 非引用字段调用 &#x3D;&#x3D; 判断</span><br><span class="line">                &#x2F;&#x2F; 引用字段调用 Objects.equals() 判断, 省去 null 值的判断</span><br><span class="line">                return Objects.equals(this.name, p.name) &amp;&amp; this.age&#x3D;&#x3D;p.age;</span><br><span class="line">            &#125;</span><br><span class="line">            return false;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure></li>
<li>优缺点：<br>  | 属性             |ArrayList|LinkedList|<br>  |——             |——     |——      |<br>  | 得到指定索引元素 | 快       |O(n) 需要重头查找        |<br>  | 尾部插入元素     | 快       | 快 |<br>  | 指定索引插入 / 删除元素 |O(n) 需要移动元素       | 不需要移动元素 |<br>  | 内存占用         | 少       | 大 |</li>
<li>Iterator 遍历 List 最为高效；实现了 Iterable 的集合类可使用 for each 遍历；</li>
</ul>
<h3 id="Set- 没有重复元素的集合"><a href="#Set- 没有重复元素的集合" class="headerlink" title="Set (没有重复元素的集合)"></a>Set (没有重复元素的集合)</h3><hr>
<ul>
<li>子类：HashSet(无序), TreeSet(它实现了 SortedSet 接口, 有序)</li>
<li>方法：  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1) 添加元素： boolean add(e)</span><br><span class="line">(2) 删除元素： boolean remove(e)</span><br><span class="line">(3) 是否包含某个元素：boolean contains(e)</span><br></pre></td></tr></table></figure></li>
<li>应用场景：在聊天软件中，发送方发送消息时，遇到网络超时后就会自动重发，因此，接收方可能会收到重复的消息，在显示给用户看的时候，需要首先去重。请练习使用 Set 去除重复的消息</li>
</ul>
<h3 id="Map- 通过 key-value 查找的映射表集合 - 不保证顺序"><a href="#Map- 通过 key-value 查找的映射表集合 - 不保证顺序" class="headerlink" title="Map (通过 key-value 查找的映射表集合, 不保证顺序)"></a>Map (通过 key-value 查找的映射表集合, 不保证顺序)</h3><hr>
<ul>
<li>子类: HashMap, HashTable, TreeMap(通过实现 Comparator 接口可以保证按照 key 排序), ConcurrentHashMap</li>
<li>方法：  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1) 放入元素：put(key, val) </span><br><span class="line">(2) 重复放入某个 key，覆盖 val 值，返回老的 val 值，如果第一次放入则返回 null</span><br><span class="line">(3) 得到元素：get(key) 如果没有则返回 null</span><br><span class="line">(4) 判断是否含有某个 key: containsKey(key)</span><br><span class="line">(5) 获取 key 的集合： map.keySet()</span><br><span class="line">(6) for-each 遍历：</span><br><span class="line">        1) for(String key: map.keySet())&#123;</span><br><span class="line">              String val &#x3D; map.get(s);</span><br><span class="line">              &#x2F;&#x2F;out(key, val)</span><br><span class="line">           &#125;</span><br><span class="line">        2) for(Map.Entry&lt;String, String&gt; entry: map.entrySet())&#123;</span><br><span class="line">              String key &#x3D; entry.getKey();</span><br><span class="line">              String val &#x3D; entry.getValue();</span><br><span class="line">              &#x2F;&#x2F;out(key, val);</span><br><span class="line">           &#125;</span><br><span class="line">(7) 自定义类中 hashCode() 方法重写：</span><br><span class="line">    int hashCode()&#123;</span><br><span class="line">        return Objects.hashCode(firstName, lastName, age) &#x2F;&#x2F; 参数为成员变量</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li>
<li>特点：<ul>
<li>常用于频繁查询内容，作为缓存 cache, 提高查询效率。</li>
<li>其内部 key 的比较采用的是 equals 方法</li>
<li>其内部 key 对应的索引计算方法 hashCode()，返回值为 int。</li>
<li>为了保证 Map 能够正常运行需要正确覆盖 equals() 和 hashCode() 方法。</li>
<li>Map 首先通过计算 key 的 hashCode()得到内部索引位置，如果不同 key 的 hashCoder()相同(哈希冲突)，再次遍历 list, 因为其内部其实存储的为 List<Entry<String, Person>&gt; 列表，不单单是一个 Person.</li>
</ul>
</li>
</ul>
<h3 id="自定义类变为可比较类型的三种方法 - 这里以 Person 自定义类举例 - 按照 age 从小到大排序"><a href="# 自定义类变为可比较类型的三种方法 - 这里以 Person 自定义类举例 - 按照 age 从小到大排序" class="headerlink" title="自定义类变为可比较类型的三种方法 (这里以 Person 自定义类举例, 按照 age 从小到大排序)"></a> 自定义类变为可比较类型的三种方法(这里以 Person 自定义类举例, 按照 age 从小到大排序)</h3><hr>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1) 自定义类实现 Comparable 接口</span><br><span class="line">class Person implements Comparable&#123;</span><br><span class="line">    String name;</span><br><span class="line">    int age;</span><br><span class="line">    public Perosn(int age, String name)&#123;</span><br><span class="line">        this.age &#x3D; age;</span><br><span class="line">        this.name &#x3D; name;</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public int compareTo(Object o)&#123;</span><br><span class="line">        if(o instanceof Person)&#123;</span><br><span class="line">            Person p &#x3D; (Person)o;</span><br><span class="line">            if(this.age&#x3D;&#x3D;p.age) return 0;</span><br><span class="line">            return this.age&lt;p.age? -1: 1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">TreeSet&lt;Person&gt; set &#x3D; new TreeSet&lt;&gt;();</span><br><span class="line">set.add(new Person(20, &quot;Bob&quot;));</span><br><span class="line">set.add(new Person(21, &quot;Aim&quot;));</span><br><span class="line">set.add(new Person(22, &quot;John&quot;));</span><br><span class="line"></span><br><span class="line">(2) 匿名函数 Comparator</span><br><span class="line">TreeSet&lt;Person&gt; set &#x3D; new TreeSet&lt;&gt;(new Comparator&lt;Person&gt;(</span><br><span class="line">    @Override</span><br><span class="line">    public int compare(Person a, Person b)&#123;</span><br><span class="line">        if(a.age&#x3D;&#x3D;b.age) return 0;</span><br><span class="line">        return a.age&lt;b.age? -1: 1;</span><br><span class="line">    &#125;</span><br><span class="line">));</span><br><span class="line"></span><br><span class="line">(3) 实现自定义类 Comparator</span><br><span class="line">class MyComparator implements Comparator&#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int compare(Object a, Object b)&#123;</span><br><span class="line">        Person a &#x3D; (Person)a;</span><br><span class="line">        Person b &#x3D; (Person)b;</span><br><span class="line">        if(a.age&#x3D;&#x3D;b.age) return 0;</span><br><span class="line">        return a.age&lt;b.age? -1: 1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Queue- 队列，-FIFO"><a href="#Queue- 队列，-FIFO" class="headerlink" title="Queue (队列， FIFO)"></a>Queue (队列， FIFO)</h3><hr>
<ul>
<li>常用实现类：  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Queue&lt;String&gt; que &#x3D; new LinkedList&lt;&gt;()</span><br><span class="line">Queue&lt;String&gt; que &#x3D; new PriorityQueue&lt;&gt;() &#x2F;&#x2F; 优先队列， 其内部元素不许实现 Comparable 接口</span><br></pre></td></tr></table></figure></li>
<li>方法：  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1) 获取队列长度：size()</span><br><span class="line">(2) 队尾添加元素：add() &#x2F; offer()  boolean</span><br><span class="line">(3) 队头删除元素：remove() &#x2F; poll() 返回元素类型</span><br><span class="line">(4) 获取队头元素但不删除： element() &#x2F; peek()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Deque- 双端队列，两端插入"><a href="#Deque- 双端队列，两端插入" class="headerlink" title="Deque (双端队列，两端插入)"></a>Deque (双端队列，两端插入)</h3><hr>
<ul>
<li>常用实现类：  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Deque&lt;String&gt; que &#x3D; new LinkedList&lt;&gt;()</span><br></pre></td></tr></table></figure></li>
<li>方法 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1) 队头 &#x2F; 队尾 添加元素：addFirst(), offerFirst() &#x2F; addLast(), offerFirst()</span><br><span class="line">(2) 队头 &#x2F; 队尾 删除元素：removeFirst(), pollFirst() &#x2F; removeLast(), pollLast()</span><br><span class="line">(3) 队头 &#x2F; 队尾 取出元素：getFirst(), peekFirst() &#x2F; getLast(), peekLast()</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>java 基础 - 面向对象</title>
    <url>/2021/03/23/java%E5%9F%BA%E7%A1%80-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/</url>
    <content><![CDATA[<h3 id="重载与重写的区别"><a href="# 重载与重写的区别" class="headerlink" title="重载与重写的区别"></a>重载与重写的区别</h3><hr>
<div class="table-container">
<table>
<thead>
<tr>
<th>类型</th>
<th>重载</th>
<th>重写</th>
</tr>
</thead>
<tbody>
<tr>
<td>方法名</td>
<td>一定不能修改</td>
<td>一定不能修改</td>
</tr>
<tr>
<td>参数列表</td>
<td>必须修改</td>
<td>一定不能修改</td>
</tr>
<tr>
<td>返回类型</td>
<td>可以修改</td>
<td>可以不同，但必须是父类返回值的派生类</td>
</tr>
<tr>
<td>访问权限</td>
<td>可以修改</td>
<td>一定不能做更加严格的显示(可以降低限制)</td>
</tr>
<tr>
<td>抛出异常</td>
<td>可以修改</td>
<td>可以减少或者删除，不能抛出新的或者更广的异常</td>
</tr>
</tbody>
</table>
</div>
<span id="more"></span>
<h3 id="抽象类和接口的区别"><a href="# 抽象类和接口的区别" class="headerlink" title="抽象类和接口的区别"></a>抽象类和接口的区别</h3><hr>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>abstract class</th>
<th>interface</th>
</tr>
</thead>
<tbody>
<tr>
<td>继承</td>
<td>只能 extends 一个类</td>
<td>可以 implements 多个接口</td>
</tr>
<tr>
<td>字段</td>
<td>可以定义实例字段</td>
<td>不能定义</td>
</tr>
<tr>
<td>抽象方法</td>
<td>可以定义抽象方法</td>
<td>均为抽象方法</td>
</tr>
<tr>
<td>非抽象方法</td>
<td>可以包含抽象方法</td>
<td>不能包含</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>抽象类中可以包含抽象方法和成员变量，但是接口只能含有抽象方法</li>
<li>它们都是本质上定义接口规范。保证所有子类都有相同的接口规范</li>
<li>一个实体类只能继承一个抽象类，但是可以实现多个接口</li>
<li>接口之间的继承使用 extends 关键词</li>
<li>接口在定义变量时，默认修饰 <code>public static final</code>，可以省去不写</li>
<li>接口在定义方法是，默认修饰 <code>public abstract</code>, 可以省去。</li>
</ul>
<h3 id="访问作用域 -private-protected-public"><a href="# 访问作用域 -private-protected-public" class="headerlink" title="访问作用域 (private, protected, public)"></a> 访问作用域(private, protected, public)</h3><hr>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>本类</th>
<th>同一个包(子类和无关类)</th>
<th>不同包下(子类)</th>
<th>不同包下(无关类)</th>
</tr>
</thead>
<tbody>
<tr>
<td>private</td>
<td>Y</td>
<td>N</td>
<td>N</td>
<td>N</td>
</tr>
<tr>
<td>default</td>
<td>Y</td>
<td>Y</td>
<td>N</td>
<td>N</td>
</tr>
<tr>
<td>protected</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>N</td>
</tr>
<tr>
<td>public</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>final<ul>
<li>定义为 final 的类，不能被继承</li>
<li>定义为 final 的方法，不能被子类重写</li>
<li>定义为 final 的成员变量，可阻止其被重写赋值</li>
<li>定义为 final 的局部变量，可阻止其被重写赋值</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>你还在用 SGD?</title>
    <url>/2021/04/07/%E4%BD%A0%E8%BF%98%E5%9C%A8%E7%94%A8SGD/</url>
    <content><![CDATA[<p>深度学习优化算法: SGD -&gt; SGDM -&gt; NAG -&gt; AdaGrad -&gt; AdaDelta -&gt; Adam -&gt; Nadam<br>下面介绍的内容都会用到的数学表达：</p>
<ul>
<li><strong>待优化参数</strong> $w$</li>
<li><strong>目标函数</strong> $f(w)$</li>
<li><strong>初始学习率</strong> $a$</li>
</ul>
<p>优化过程可以建模为, 在每个 epoch $t$ 中：</p>
<ul>
<li><ol>
<li>计算目标函数关于当前参数梯度： $g_t = \nabla f(w_t)$</li>
</ol>
</li>
<li><ol start="2">
<li>根据历史梯度计算 <strong>一阶动量 </strong> 和 <strong> 二阶动量</strong>：$m_t=\phi(g_1,…,g_t)$, $V_t=\varphi(g_1,…,g_t)$</li>
</ol>
</li>
<li><ol start="3">
<li>计算当前时刻的下降梯度：$\eta_t = a \cdot m_t / \sqrt{V_t}$</li>
</ol>
</li>
<li><ol start="4">
<li>根据下降梯度进行更新：$w_{t+1} = w_t - \eta_t$<h4 id="1-SGD-stochastic-gradient-descent- 随机梯度下降"><a href="#1-SGD-stochastic-gradient-descent- 随机梯度下降" class="headerlink" title="1. SGD (stochastic gradient descent) 随机梯度下降"></a>1. SGD (stochastic gradient descent) 随机梯度下降</h4>SGD 没有动量的概率，即 $m_t=g_t, V_t=I^2$, 即 $\eta_t=a\cdot g_t$</li>
</ol>
</li>
<li>缺点：下降速度慢，可能在沟壑的两边持续政党，停留在局部最优点。</li>
</ul>
<h4 id="2-SGDM-stochastic-gradident-descent-with-momentum"><a href="#2-SGDM-stochastic-gradident-descent-with-momentum" class="headerlink" title="2. SGDM (stochastic gradident descent with momentum)"></a>2. SGDM (stochastic gradident descent with momentum)</h4><p>为了抑制 SGD 的震荡，SGDM 认为梯度下降过程加入了惯性，下坡的实收，如果发现是陡坡，那么惯性下降更快。<br>$$m_t = \beta_1m_{t-1} + (1-\beta_1)g_t$$<br>一阶动量是各个时刻梯度方向的指数移动平均值，约等于 <strong>$1/(1-\beta_1)$</strong> 个最近时刻的梯度方法的平均值。</p>
<span id="more"></span>
<h4 id="3-3-SGD-with-Nesterov-Acceleration"><a href="#3-3-SGD-with-Nesterov-Acceleration" class="headerlink" title="3. 3 SGD with Nesterov Acceleration"></a>3. 3 SGD with Nesterov Acceleration</h4><p>NAG 全称 Nesterov Accelerated Gradient, 是在 SGD, SGD-M 的基础上进行了改进，主要是在 <strong>步骤 1</strong> 上进行了改进。根据上面公式，时刻 $t$ 的主要下降方向主要由 <strong>累计动量 </strong> 决定，t 时刻的 <strong> 梯度方向 </strong> 说了不算，为此可以先看看跟着累计动量走一步的结果，NAG 主要在步骤 1 中 <strong> 不计算当前位置的梯度方向</strong>，而是按照累积动量走一步。<br>$$g_t = \nabla f(w_t - a \cdot m_{t-1} / \sqrt{V_{t-1}})$$</p>
<h4 id="4-AdaGrad"><a href="#4-AdaGrad" class="headerlink" title="4. AdaGrad"></a>4. AdaGrad</h4><p>二阶动量的到来，意味着 <strong>自适应学习率</strong> 优化算法的到来。SGD 及其变体以同样的学习率更新每一个参数，而深度神经网络包含大量的参数，这些参数并不是总会得到，对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太多，希望学习速率慢一点；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然的样本上学习一些，即学习率大一些。</p>
<ul>
<li><p>度量历史更新频率：二阶动量表示所有梯度值的平方和： $V_t = \sum_t g_t^2$</p>
</li>
<li><p>$\eta_t = a \cdot m_t / \sqrt{V_t}$, 参数更新越频繁，二阶动量越大，学习率越小。</p>
</li>
<li><p>缺点是 $V_t$ 是单调增加的，那么学习率可能减少到 0， 使得训练提前结束。</p>
</li>
</ul>
<h4 id="5-AdaDelta-RMSProp"><a href="#5-AdaDelta-RMSProp" class="headerlink" title="5. AdaDelta / RMSProp"></a>5. AdaDelta / RMSProp</h4><p>改进 AdaGrad 的学习率变化过于激进的缺点，思路是并不累积所有历史提取，而是关注于过去一段时间窗口的下降梯度。<br>$$V_t = \beta_2 V_{t-1} + (1 - \beta_2) g_t^2$$<br>避免二阶动量持续累积，导致训练提前结束的问题了。</p>
<h4 id="6-Adam"><a href="#6-Adam" class="headerlink" title="6. Adam"></a>6. Adam</h4><ul>
<li><p><strong>实际的下降方法</strong>：$m_t$;</p>
</li>
<li><p><strong>实际的学习速率</strong>：$a / \sqrt{V_t}$</p>
</li>
<li><p>SGD-M 在 SGD 上考虑了一阶动量， AdaGrad, AdaDelta 在 SGD 上考虑了二阶动量， 把一阶动量和二阶动量都用起来，就是 Adam（Adapative Momentum）。<br>$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t $$<br>$$V_t = \beta_2 V_{t-1} + (1 - \beta_2) g_t^2 $$</p>
</li>
<li><p>缺点：</p>
<ul>
<li><p>可能会不收敛的情况。二阶动量 $V_t$ 在学习的过程中时大时小，产生震荡。学习率递减策略。</p>
</li>
<li><p>可能错过全局最优解。主要是 Adam 后期学习率太低了，影响了有效的收敛。</p>
</li>
</ul>
</li>
</ul>
<h4 id="7-NAdam-Nesterov-Adam"><a href="#7-NAdam-Nesterov-Adam" class="headerlink" title="7. NAdam (Nesterov + Adam)"></a>7. NAdam (Nesterov + Adam)</h4><p>$$g_t = \nabla f(w_t - m_{t-1}/\sqrt{V_t})$$</p>
<h4 id="8-RAdam"><a href="#8-RAdam" class="headerlink" title="8. RAdam"></a>8. RAdam</h4><p>它能够根据方差分散度，动态地打开或者关闭自适应学习率，提供一种不需要可调节参数学习率预热的方法，它兼顾了 Adam 和 SGD 两者的有点，即能保证收敛速度快，也不容易陷入局部最优解，在较大学习率的情况下，精度甚至优于 SGD。</p>
<h4 id="9-Lookahead"><a href="#9-Lookahead" class="headerlink" title="9. Lookahead"></a>9. Lookahead</h4><ul>
<li><p>lookahead 与 其他最优化器是正交的，意味着可以使用 lookahead 加强现有最优化方法的性能</p>
</li>
<li><p>它能够有效的降低方差</p>
</li>
<li><p>Lookahead 是怎么做的</p>
<ul>
<li><p>lookahead 会迭代地更新两组权重，slow weights $\phi$ 和 fast weights $\varPhi$, 前者在后者每更新 k 次后更新一次。lookahead 可以将任意标准优化算法 A 作为内部优化器来更新 fast weights.</p>
</li>
<li><p>内部优化器 A 进行 k 次更新后，Lookahead 在权重空间 $\phi - \varPhi$ 中进行线性插值更新 slow weights。方向为最后一个 fast weights.</p>
</li>
<li><p>slow weights 每更新一次， fast weights 将被重置尾目前的 slow weights 值。<br>具体算法如下图所示</p>
<img src="/imgs/bmjqxx/lookahead.png" style="width:80%;height:60%;text-align:center"></li>
</ul>
</li>
</ul>
<h4 id="10-Warmup- 策略"><a href="#10-Warmup- 策略" class="headerlink" title="10. Warmup 策略"></a>10. Warmup 策略 </h4><p> 热身，在模型刚开始训练的时候是一个非常小的学习率，紧接着模型训练学习率慢慢变大。<br>有效原因：</p>
<ul>
<li><p>有助于减缓模型在初始阶段对 mini-batch 的 <strong>提前过拟合</strong>，保持分布的平稳。</p>
</li>
<li><p>有助于保持模型深层的稳定性。</p>
</li>
<li><p>在刚开始训练的时候，模型的权重是随机初始化的，选择一个较大的学习率，可能带来模型的不稳定（震荡）。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>热爱生命</title>
    <url>/2021/03/11/%E7%83%AD%E7%88%B1%E7%94%9F%E5%91%BD/</url>
    <content><![CDATA[<center>
<h1> 热爱生命 </h1>
<p>
我不去想，<br>
是否能够成功 ，<br>
既然选择了远方 ，<br>
便只顾风雨兼程。<br>
</p>
<span id="more"></span>
<p>
我不去想，<br>
能否赢得爱情 ，<br>
既然钟情于玫瑰 ，<br>
就勇敢地吐露真诚 。<br>
</p>

<p>
我不去想，<br>
身后会不会袭来寒风冷雨 ，<br>
既然目标是地平线，<br>
留给世界的只能是背影 。<br>
</p>

<p>
我不去想，<br>
未来是平坦还是泥泞 ，<br>
只要热爱生命 </font>，<br>
一切，都在意料之中。
</p>
</center>]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>诗歌</tag>
      </tags>
  </entry>
  <entry>
    <title>百面机器学习 (一)</title>
    <url>/2021/03/28/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E4%B8%80)/</url>
    <content><![CDATA[<p>-</p>
<h4 id="1- 为什么需要对于数值类型特征进行特征归一化"><a href="#1- 为什么需要对于数值类型特征进行特征归一化" class="headerlink" title="1. 为什么需要对于数值类型特征进行特征归一化?"></a>1. 为什么需要对于数值类型特征进行特征归一化?</h4><hr>
<p>为了消除不同数据特征之间的量纲影响，我们需要对于特征进行归一化处理，是的不同指标之间具有可比性。其主要方法为以下两种:</p>
<ul>
<li><p>线性函数归一化 (Min-Max Scaling)：对原始特征进行线性变换，归一化到[0,1] 范围内。</p>
<script type="math/tex; mode=display">X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}</script></li>
<li><p>零均值归一化：会将原始特征映射到均值为 0，方差为 1 的分布上。设原始特征的均值和方差分别为 $\mu, \delta$，归一化公式。</p>
<script type="math/tex; mode=display">z = \frac{x - \mu}{\delta}</script><span id="more"></span>
<p>Python 代码实现</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 线性函数归一化, 零均值归一化</span><br><span class="line">from sklearn.preprocessing import MinMaxScaler, StandardScaler</span><br><span class="line">scaler &#x3D; MinMaxScaler()</span><br><span class="line">x &#x3D; scaler.fit(x)</span><br><span class="line">x &#x3D; scaler.transform(x)</span><br></pre></td></tr></table></figure>
<p>好处：归一化后能够加快模型的梯度下降收敛速度（线性回归，逻辑回归，SVM, NN），树模型不适用。<br><img src="/imgs/bmjqxx/1-1.png" style="width:80%;height:80%;text-align:center"></p>
</li>
</ul>
<h4 id="2- 在对数据预处理时，如何处理类别型特征？"><a href="#2- 在对数据预处理时，如何处理类别型特征？" class="headerlink" title="2. 在对数据预处理时，如何处理类别型特征？"></a>2. 在对数据预处理时，如何处理类别型特征？</h4><hr>
<ul>
<li><p>序号编码：类别之间具有大小关系。例如乘积的高、中、低三挡。序号编码按照大小关系对类别特征赋予一个数值 id。</p>
</li>
<li><p>独热编码：类别间不具有大小关系的特征。需要注意类别取值较多的情况下的问题</p>
<ul>
<li>使用 稀疏向量节省空间。</li>
<li>进行特征选择降低维度。</li>
</ul>
</li>
<li><p>二进制编码：先使用序号编码为每一个类别赋予一个类别 id, 然后使用二进制编码 hash。</p>
</li>
</ul>
<h4 id="3- 什么是组合特征？如何处理高维组合特征？"><a href="#3- 什么是组合特征？如何处理高维组合特征？" class="headerlink" title="3. 什么是组合特征？如何处理高维组合特征？"></a>3. 什么是组合特征？如何处理高维组合特征？</h4><hr>
<p>为了提高复杂关系的拟合能力，特征工程经常将 <strong> 一阶离散特征 </strong> 两两组合，构成高阶组合特征。例如广告点击预估问题中，原始数据中有语言和类型两种离散特征。</p>
<script type="math/tex; mode=display">Y = sigmoid(\sum_i\sum_jW_{ij}<x_i, x_j>)</script><p>$W_{ij}$ 的维度等于 $|x_i| \cdot |x_j|$, 即为 $i$ 和 $j$ 的特征个数乘积。</p>
<ul>
<li><p>可能存在的问题：当 <strong> 离散型 </strong> 特征为 id 类型的特征时候，例如推荐问题中, m 个用户 id 和 n 个物品 id。那么 m 和 n 都是千亿级别的，这个乘积就非常大，参数量就会非常大。一种行之有效的方法就是将用户和物品分别映射为 $k$ 维度的低纬度向量 ($k\ll m, k\ll n$), 此时需要学习的参数规模变为 $m \times k + n \times k$。 </p>
</li>
<li><p>其他的特征组合方式： 两两组合特征仍然存在过拟合的问题，其他特征组合方式，基于决策树的特征组合寻找方式。<br><img src="/imgs/bmjqxx/1-2.png" style="width:80%;height:80%;text-align:center"></p>
</li>
</ul>
<h4 id="4- 文本表示模型有哪些？各自的优缺点？"><a href="#4- 文本表示模型有哪些？各自的优缺点？" class="headerlink" title="4. 文本表示模型有哪些？各自的优缺点？"></a>4. 文本表示模型有哪些？各自的优缺点？</h4><hr>
<ul>
<li><p>词袋模型：每篇文本看做一个忽略顺序的词语集合，被表示为一个长向量。向量的每一维表示一个词语，维度对应的权重表示该词在原文章中的重要程度。TF-IDF 计算公式。</p>
<script type="math/tex; mode=display">TF-IDF (t, d) = TF (t, d) \times IDF (t)</script><p>其中 $TF (t, d)$ 表示 $t$ 单词在文档 $d$ 中出现的频率。 $IDF (t)$ 表示逆文档频率。</p>
<script type="math/tex; mode=display">IDF (t) = \log \frac{文章总数}{出现 t 的文章数 + 1}</script></li>
<li><p>N-gram: 单个拆分单词可能忽略到连续单词表示的信息。 $n-gram$ 则表示组合连续的词语为一个特征。</p>
</li>
<li><p>主题模型：从文本库中选择具有代表性的主题（得到每个主题上次的分布特性）。并且计算每篇文章的主题分布。</p>
</li>
<li><p>词嵌入和深度学习模型：词向量化模型的统称。核心思想是将每一个词映射成低纬度空间上的一个稠密向量。</p>
</li>
</ul>
<h4 id="5-Word2Vec- 是如何工作的？它和 -LDA- 有什么区别和联系？"><a href="#5-Word2Vec- 是如何工作的？它和 -LDA- 有什么区别和联系？" class="headerlink" title="5. Word2Vec 是如何工作的？它和 LDA 有什么区别和联系？"></a>5. Word2Vec 是如何工作的？它和 LDA 有什么区别和联系？</h4><hr>
<ul>
<li><p>CBOW 是根据上下文出现的词语来预测当前词的生成概率。Skip-gram 则是根据当前词预测上下文中各个词的生成概率。激活函数使用 $softmax$。<br><img src="/imgs/bmjqxx/1-3.png" style="width:80%;height:80%;text-align:center"></p>
</li>
<li><p>由于 CBOW 和 Skip-gram 中激活函数存在归一化的缘故，每次迭代过程非常缓慢。 产生 Hierarchical Softmax 和 Negative Sampling 两种改进方法。</p>
</li>
<li><p>Hierarchical Softmax 改机思路: 在 CBOW 和 Skip-gram 的最后一个 softmax 相当于一个多分类。它需需要对语料库中每个单词（类）计算指数概率并计算归一化，在几十万词汇量的预料是非常消耗的。</p>
<ul>
<li>首先根据词频构造 Huffman 树，每个词都处于树上的某个叶子节点。词频较大的词语出现在浅层的中，较小的出现在较深的叶子节点。</li>
<li>将原本的 $|V|$ 分类问题转变为 $\log|V|$ 个二分类问题（本质上是一个 LR 分类器）。</li>
</ul>
</li>
<li><p>Negative Sampling: CBOW 框架的简单原理，负采样遍历到每一个目标词，然后让目标词的概率 $P (w_t | c_t)$ 最大，由 softmax 公式原理，让分子中 $e^{(w_t)^T\cdot x}$ 最大，分母中其他非目标词 $e^{(w_i)^T \cdot x}$ 最小。</p>
<ul>
<li>softmax 的计算量大是因为它将所有其他非目标词语都当做了负样本。</li>
<li>负采样的思想，每次按照一定概率随机采样一些词当负例。<script type="math/tex; mode=display">P(w_t | c_t) = \frac{exp(X_j)}{\sum_{i=1}^{K}exp(X_i)}</script></li>
<li>将原本的 $|V|$ 分类问题变成了 K 分类问题。</li>
</ul>
</li>
<li><p>LDA: 利用文档中单词的共现关系对单词按主题聚类。将”<strong>文档 - 单词 </strong>“ 拆解为 “<strong> 文档 - 主题 </strong>“ 和 “<strong> 主题 - 单词 </strong>“两个概率分布。(<a href="https://zhuanlan.zhihu.com/p/31470216"> 一文详解 LDA 模型</a>)</p>
<ul>
<li><p>二项分布：每次概率投掷只有 0 or 1 两种值，重复 n 次比较最后的结果</p>
<script type="math/tex; mode=display">P(K = k) = C_n^k \cdot p^k \cdot (1 - p)^{(n - k)}</script></li>
<li><p>多项式分布：每次投掷都有概率为 $p_1, p_2, …, p_n$ 种可能。</p>
<script type="math/tex; mode=display">P(x_1, ..., x_n ; n, p_1, ..., p_n) = \frac{n!}{x_1! \cdot ... \cdot x_n!}p_1^{x_1} \cdot ... \cdot p_n^{x_n}</script></li>
<li><p>共轭先验分布：先验分布 $p(\theta)$ 和后验分布 $p(\theta | x)$ 满足同样的分布律，则它们叫做共轭分布。</p>
</li>
</ul>
</li>
</ul>
<h4 id="6- 图像分类任务中，数据不足可能产生的问题？如何缓解数据量不足带来的问题？"><a href="#6- 图像分类任务中，数据不足可能产生的问题？如何缓解数据量不足带来的问题？" class="headerlink" title="6. 图像分类任务中，数据不足可能产生的问题？如何缓解数据量不足带来的问题？"></a>6. 图像分类任务中，数据不足可能产生的问题？如何缓解数据量不足带来的问题？</h4><hr>
<ul>
<li><p>数据不足会带来过拟合问题：通过简化模型（非线性模型变为线性模型），添加参数约束项缩小假设空间（L1/L2 正则化），集成学习，Dropout(降低)参数。</p>
</li>
<li><p>根据先验知识，在保持特定信息前提下，扩充数据集。</p>
<ul>
<li>一定程度内的旋转，平移，裁剪，反转等。</li>
<li>对像素添加噪声扰动，椒盐噪声，高斯白噪声等。</li>
<li>颜色变换，RGB 颜色空间进行主成分分析。</li>
<li>改变图片亮度，清晰度、对比度和锐度等</li>
</ul>
</li>
<li><p>生成对抗网络</p>
</li>
<li><p>借助其他模型或数据进行迁移学习。（基于大数据上的预训练的通用模型，在该小数据上 fine-tune）。</p>
</li>
<li><p>AUC 指标一般不会随着正负样本比例变化，而 AUPC 和 P-R (Precision-Recall) 曲线则会受到正负样本比例的影响。</p>
</li>
<li><p>欧式距离体现了数值上的 <strong>绝对差异 </strong>，余弦距离体现了方向上的 <strong> 相对差异</strong>。</p>
<ul>
<li><p>当去判断为 A 和 B 两个用户在观看类型的向量时候 A(0, 1), B(1, 0)，此时 <strong>余弦距离 &gt; 欧式距离</strong>，而此时更加关注用户对不同视频的喜好，关注相对差异，应该使用余弦距离。</p>
</li>
<li><p>当分析用户活跃度, 使用登陆次数和观看时长作为特征。A(1, 10), B(10, 100)，此时 <strong>余弦距离 &lt; 欧式距离</strong>。但是应该使用欧式距离，更加关注与绝对差异。</p>
</li>
</ul>
</li>
</ul>
<h4 id="7- 超参数有哪些调优方法？"><a href="#7- 超参数有哪些调优方法？" class="headerlink" title="7. 超参数有哪些调优方法？"></a>7. 超参数有哪些调优方法？</h4><hr>
<ul>
<li><p>网格搜索： 它通过查找搜索范围内的所有点来确定最优值。缺点是非常需要消耗资源和时间。实际应用中网格搜索法一般先使用较为广的搜索范围和较大步长寻找全局最优值可能的位置，然后在缩小范围和步长进行精确查找。</p>
</li>
<li><p>随机搜索：在搜索范围随机选择样本点。理论依据，如果样本点足够多，那么随机采样很大概率可以找到全局最优解或近似解，它比网格搜索更快。</p>
</li>
<li><p>贝叶斯优化算法：对目标函数进行学习，找到目标函数向全局最优值提升的参数。缺点是容易陷入局部最优值。</p>
</li>
</ul>
<h4 id="8- 集中降低过拟合和欠拟合风险的方法？"><a href="#8- 集中降低过拟合和欠拟合风险的方法？" class="headerlink" title="8. 集中降低过拟合和欠拟合风险的方法？"></a>8. 集中降低过拟合和欠拟合风险的方法？</h4><hr>
<ul>
<li><p>降低 “过拟合” 风险的方法。</p>
<ul>
<li><p>获得更多的训练数据。</p>
</li>
<li><p>降低模型的复杂度（NN 减少网络层数，决策树降低树的深度、进行剪枝操作）。</p>
</li>
<li><p>正则化方法（L1 / L2 正则化）。</p>
</li>
<li><p>集成学习方法： 多个模型集成在一起，降低单个模型的过拟合风险。</p>
</li>
</ul>
</li>
<li><p>降低 “欠拟合” 风险的方法：</p>
<ul>
<li><p>添加新的特征。</p>
</li>
<li><p>增加模型的复杂度。</p>
</li>
<li><p>减少正则化系数。</p>
</li>
</ul>
</li>
</ul>
<h4 id="9-SVM"><a href="#9-SVM" class="headerlink" title="9. SVM"></a>9. SVM</h4><hr>
<ul>
<li><p>svm 中两类分类点在超平面上的投影是线性不可分的</p>
</li>
<li><p>$R^d$ 空间中的某个点 $p \in R^d$ 到超平面 $w^T \cdot x + b = 0$ 的距离为 $\frac{|w^T \cdot p + b|}{||w||}$ (点 p 到超平面的距离等于 p 与超平面上某点想超平面的法向量的投影)</p>
</li>
<li><p>间隔 $\gamma$ 的定义: 间隔表示距离超平面最近的样本到划分超平面距离的两倍。<br>$\gamma := 2min_i \frac{|w^T \cdot x_i + b|}{||w||}$</p>
</li>
<li><p>定理 3：线性支持向量机的目标, 找到一组合适的 $w, b$，使得</p>
<script type="math/tex; mode=display">\max\limits_{w,b}\min\limits_{i}\frac{2|{w^T\cdot x_i + b|}}{||w||}</script><script type="math/tex; mode=display">y_i(w^T\cdot x_i + b) > 0, i = 1,...,m</script></li>
<li><p>定理 4：若 $(w^<em>, b^</em>)$ 是线性支持向量的一个解，那么对于任意的 $r&gt;0$ 仍然是该优化问题的解。因此为了简化优化问题，我们约束 $(w, b)$ 使得 $\min\limits_i|w^T\cdot x_i + b| = 1$</p>
</li>
<li><p>定理 5 (线性支持向量机基本型)：根据定理 3, 4 推导</p>
<script type="math/tex; mode=display">\max\limits_{w, b}\frac{2}{||w||}</script><script type="math/tex; mode=display">y_i (w_T\cdot x_i + b) > 0, i=1,...,m</script><script type="math/tex; mode=display">\min\limits_i |w^T \cdot x_i + b| = 1</script><p>转化为：其中在支持向量上的点取到等于符号</p>
<script type="math/tex; mode=display">\min\limits_{w, b}\frac{1}{2}w^T\cdot w</script><script type="math/tex; mode=display">y_i (w_T\cdot x_i + b) \geq 1, i=1,..., m</script></li>
<li><p>定义 6 拉格朗日函数，对于优化问题</p>
<script type="math/tex; mode=display">\min\limits_u f(u)</script><script type="math/tex; mode=display">g_i(u) \leq 0, i=1,..., m</script><script type="math/tex; mode=display">h_j(u) = 0, j=1,..., n</script><p>拉格朗日函数形式为:</p>
<script type="math/tex; mode=display">L(u, \alpha, \beta) = f(u) + \sum_i\alpha_ig_i(u) + \sum_j\beta_j(u)</script><script type="math/tex; mode=display">\alpha_i \geq 0</script></li>
<li><p>推理 7. 结合定理 5 和 6，优化公式：</p>
<script type="math/tex; mode=display">\min\limits_{u}\max\limits_{\alpha, \beta} L(u, \alpha, \beta)</script><script type="math/tex; mode=display">\alpha_i \geq 0, i = 1,..., m.</script></li>
<li><p>KKT 条件（推理 7）描述的优化为题在最优值处必须满足如下条件：</p>
<ul>
<li><p>主问题可行: $g_i(u) \leq 0, h_j(u) = 0$。</p>
</li>
<li><p>对偶问题可行: $\alpha_i \geq 0$；</p>
</li>
<li><p>互补松弛条件: $a_ig_i(u) = 0$;</p>
</li>
</ul>
</li>
<li><p>对偶问题(推理 7 优化的对偶问题)， 且对偶问题是主问题（推理 7）的下界。</p>
<script type="math/tex; mode=display">\max_{\alpha, \beta}\min_{u} L(u, \alpha, \beta)</script><script type="math/tex; mode=display">\alpha_i \geq 0, i = 1, ..., m</script><script type="math/tex; mode=display">\max_{\alpha, \beta}\min_{u} L(u, \alpha, \beta) \leq \min_{u}\max_{\alpha, \beta} L(u, \alpha, \beta)</script></li>
<li><p>线性支持向量机的拉格朗日形式：</p>
<script type="math/tex; mode=display">L(w, b, a) = \frac{1}{2}w^Tw + \sum_{i=1}^m a_i[1 - y_i(w^T \cdot x_i + b)]</script><script type="math/tex; mode=display">a_i \geq 0, i=1, ..., m</script><p>其对偶问题为:</p>
<script type="math/tex; mode=display">\max_a\min_{w, b} \frac{1}{2}w^Tw + \sum_{i=1}^m a_i[1 - y_i(w^T \cdot x_i + b)]</script><script type="math/tex; mode=display">a_i \geq 0, i=1, ..., m</script></li>
<li><p>线性支持向量机的对偶问题等价于找到一组合适的参数 a, 使得。</p>
<script type="math/tex; mode=display">\min_a \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^ma_ia_jy_iy_jx_i^Tx_j - \sum_{i-1}^ma_i</script><script type="math/tex; mode=display">\sum_{i=1}^ma_iy_i = 0</script><script type="math/tex; mode=display">a_i \geq 0, i=1, ..., m</script><p>根据内层对 $(w, b)$ 求导，然后令偏导等于 0， 得到（w, b）的最优值。</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial w}=0 \to w = \sum_{i=1}^ma_iy_ix_i</script><script type="math/tex; mode=display">\frac{\partial L}{\partial b}=0 \to     \sum_{i=1}^ma_iy_i = 0</script></li>
<li><p>线性支持向量机的 KKT 条件：</p>
<ul>
<li><p>主问题可行：  $1 - y_i(w^Tx_i + b) \leq 0$</p>
</li>
<li><p>对偶问题可行： $a_i \geq 0$</p>
</li>
<li><p>互补松弛: $a_i (1 - y_i(w^Tx_i + b)) = 0$</p>
</li>
</ul>
</li>
<li><p>引理 1: 线性支持向量机中，支持向量是距离划分平面最近的样本，落在最大间隔边界上。<br>由 KKT 条件：</p>
<ul>
<li><p>$a_i &gt; 0$ 时候，$1 - y_i(w^Tx_i + b) = 0 \to y_i(w^Tx_i + b) = 1$</p>
</li>
<li><p>$a_i = 0$ 时候, $1 - y_i(w^Tx_i + b) &lt; 0 \to y_i(w^Tx_i + b) &gt; 1$</p>
</li>
</ul>
</li>
<li><p>引理 2: 支持向量机的参数 $(w, b)$ 仅由支持向量决定，与其他样本无关（只有 $a_i &gt; 0$ 样本有效）。</p>
<script type="math/tex; mode=display">w = \sum_{i=1}^m a_iy_ix_i</script></li>
<li><p>常见核函数：</p>
<ul>
<li><ol>
<li>当特征维度 d 超过样本数，使用线性核。</li>
</ol>
</li>
<li><ol>
<li>当特征维度 d 较小，样本数 m 中等时，使用 RBF 核。</li>
</ol>
</li>
<li><ol>
<li>当特征维度 d 较小，样本数 m 较大时，svm 性能不如神经网络。</li>
</ol>
</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>name</th>
<th>experssion</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>线性核函数</td>
<td>$k(x_i, x_j) = x_i * x_j$</td>
<td>有高效实现，不容易过拟合</td>
<td>无法解决非线性可分问题</td>
</tr>
<tr>
<td>多项式核函数</td>
<td>$k(x_i, x_j) = ((x_i * x_j) + 1) ^ d$</td>
<td>比线性核更一般，d 描述了被映射空间的复杂度</td>
<td>参数多，d 很大时计算不稳定</td>
</tr>
<tr>
<td>高斯核函数（RBF）</td>
<td>$k(x_i, x_j) = exp(- \frac{(x_i-x_j)^2}{2{\delta}^2})$</td>
<td>只有一个参数，没有计算不稳定问题</td>
<td>计算慢，过拟合风险大</td>
</tr>
<tr>
<td>sigmoid 核函数</td>
<td>$k(x_i, x_j) = tanh(\eta <x_i, x_j> + \theta)$</td>
<td>实现的是一种多层神经网络</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><p>软间隔: 前面的公式都是假设样本完全是线性可分的（虽然理论上总能找到一个高维映射使数据线性可分。但是实际任务中找到这样的核函数是非常难的）</p>
</li>
<li><p>软间隔支持向量机基本类型（离散型）：在优化间隔的同时，允许分类错误的样本出现，但是这类样本需要尽可能少。</p>
<script type="math/tex; mode=display">\min_{w,b}\frac{1}{2}w^Tw + C\sum_{i=1}^mI(y_i \neq sign(w^T\phi(x_i) + b))</script><script type="math/tex; mode=display">y_i(w^T\phi(x_i) + b) \geq 1, if y_i = sign(w^T\phi(x_i) + b)</script><p>$C$ 是个可调节参数用于权衡优化间隔的超参数。</p>
</li>
<li><p>软间隔支持向量机基本型（连续型）：为了使优化函数问题保持为二次规划问题，引入连续值变量（松弛变量 $\xi_i$ ) 用以度量样本违背约束的程度。</p>
<ul>
<li><p>当 $y_i(w^T\phi(x_i) + b) \geq 1$, $\xi_i = 0$</p>
</li>
<li><p>否则, $\xi_i = 1 - y_i(w^T\phi(x_i) + b)$ (hinge loss)</p>
</li>
</ul>
<script type="math/tex; mode=display">\min_{w, b, \xi}\frac{1}{2}w^Tw + C \sum_{i=1}^m \xi_i</script><script type="math/tex; mode=display">y_i(w^T\phi(x_i) + b) \geq 1 - \xi_i, i = 1, ..., m</script><script type="math/tex; mode=display">\xi_i \geq 0, i = 1, ..., m</script></li>
<li><p>软间隔支持向量机对偶型：其对偶问题等价于找到一组合适的 a， 使得</p>
<script type="math/tex; mode=display">\min_a\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^ma_ia_jy_iy_j\phi(x_i)^T\phi(x_j) - \sum_{i=1}^ma_i</script><script type="math/tex; mode=display">\sum_{i=1}^ma_iy_i = 0</script><script type="math/tex; mode=display">0 \leq a_i \leq C , i = 1, ..., m</script></li>
<li><p>软间隔支持向量机的拉格朗日函数:</p>
<script type="math/tex; mode=display">L(w, b, \xi, a, \beta) = \frac{1}{2}w^Tw + C\sum_{i=1}^m\xi_i + \sum_{i=1}^ma_i(1-\xi_i-y_i(w^T\phi(x_i)+b))+\sum_{i=1}^m\beta_i(-\xi_i)</script><p>对偶问题：</p>
<script type="math/tex; mode=display">\max_{a, \beta}\min_{w, b, \xi} L(w, b, \xi, a, \xi)</script><script type="math/tex; mode=display">a_i \geq 0, i = 1, ..., m</script><script type="math/tex; mode=display">\beta_i \geq 0, i = 1, ..., m</script></li>
</ul>
<p>  内层对 $(w, b, \xi)$ 求导并令偏导等于 0.</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial w} = 0 \to w = \sum_{i=1}^ma_iy_i\phi(x_i)</script><script type="math/tex; mode=display">\frac{\partial L}{\partial b} = 0 \to \sum_{i=1}^ma_iy_i = 0</script><script type="math/tex; mode=display">\frac{\partial L}{\partial \xi} = 0 \to a_i + \beta_i = C</script><script type="math/tex; mode=display">\beta_i = C - a_i \geq 0 \to 0 \leq a_i \leq C</script>]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>百面机器学习 (二)</title>
    <url>/2021/03/30/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(%E4%BA%8C)/</url>
    <content><![CDATA[<h4 id="1- 决策树之间的区别"><a href="#1- 决策树之间的区别" class="headerlink" title="1. 决策树之间的区别"></a>1. 决策树之间的区别</h4><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>ID3</th>
<th>C4.5</th>
<th>CART</th>
</tr>
</thead>
<tbody>
<tr>
<td>依据</td>
<td>信息增益</td>
<td>信息增益率</td>
<td>Gini 系数</td>
</tr>
<tr>
<td>任务</td>
<td>分类</td>
<td>分类</td>
<td>分类和回归</td>
</tr>
<tr>
<td>树类型</td>
<td>多叉树</td>
<td>多叉树</td>
<td>二叉树</td>
</tr>
</tbody>
</table>
</div>
<span id="more"></span>
<h4 id="2- 如何对决策树进行剪枝"><a href="#2- 如何对决策树进行剪枝" class="headerlink" title="2. 如何对决策树进行剪枝"></a>2. 如何对决策树进行剪枝 </h4><p> 决策树剪枝包含 2 种：</p>
<ul>
<li><p>预剪枝: 在生成决策树的过程中提前停止树的增长，核心思想是在树进行扩展之前，计算当前划分能够带来模型泛化能力的提升，如果不能则停止生长。</p>
<ul>
<li>当树达到一定深度时候，停止生长。</li>
<li>当前节点的样本数量小于某个阈值的时候，停止生长。</li>
<li>计算每次分裂对于测试集的准确度的提升，当小于某个阈值时，停止生长。</li>
</ul>
</li>
<li><p>后剪枝：在已经生成的过拟合决策树上进行剪枝。核心思想在树完全生长后，从底层向上层计算是否剪枝。剪枝过程将子树删除，叶子节点进行代替。后剪枝可用测试集的准确率判断。</p>
<ul>
<li>常见后剪枝方法：降低剪枝 (REP)， 悲观剪枝（PEP），<strong> 代价复杂度剪枝（CCP）</strong>，最小误差剪枝（MEP）。</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>预剪枝</td>
<td>简单高效，适合大规模问题</td>
<td>阈值选择，可能存在欠拟合风险，测试集可能在之后划分显著提高，但是提前终止了生长</td>
</tr>
<tr>
<td>后剪枝</td>
<td>相比于预剪枝，可得到泛化强的决策树</td>
<td>时间开销大</td>
</tr>
</tbody>
</table>
</div>
<h4 id="3- 如何定义主成分？从该定义出发，如何设计目标函数达到降维提取主成分的目的？针对该目标函数，如何对 PCA 问题求解？"><a href="#3- 如何定义主成分？从该定义出发，如何设计目标函数达到降维提取主成分的目的？针对该目标函数，如何对 PCA 问题求解？" class="headerlink" title="3. 如何定义主成分？从该定义出发，如何设计目标函数达到降维提取主成分的目的？针对该目标函数，如何对 PCA 问题求解？"></a>3. 如何定义主成分？从该定义出发，如何设计目标函数达到降维提取主成分的目的？针对该目标函数，如何对 PCA 问题求解？</h4><p>给定数据向量 ${v_1, …, v_n}$, 以二维坐标为例如下两个图的变化过程。<br><img src="/imgs/bmjqxx/4-1a.png" style="width:80%;height:80%;text-align:center"><br><img src="/imgs/bmjqxx/4-1b.png" style="width:80%;height:80%;text-align:center"></p>
<ul>
<li><p>主要目标：降维后图 b 中的黄轴就是主成分的方向。其特点是黄轴上数据更为分散，即方差更大。由此可以看出 PCA 的主要目标，寻找到最大化投影方差。</p>
</li>
<li><p>主要目标（数学表达）：对所有样本去中心化 ${x<em>1, …, x_n} = {v_1-\mu, …, v_n-\mu}$，其中 $\mu = \frac{1}{n}\sum</em>{i=1}^nv_i$, 某个向量在某个单位向量上的投影公式 $x_i^Tw = |x_i|\cdot|w|\cdot\cos(\theta) = |x_i|\cos(\theta)$, 需要让该方差最大。 </p>
<script type="math/tex; mode=display">D(x) = \frac{1}{n}\sum_{i=1}^n(x_i^Tw)^2 = \frac{1}{n}\sum_{i=1}^n(x_i^Tw)^T(x_i^Tw) = w^T(\frac{1}{n}\sum_{i=1}^nx_ix_i^T)w</script><script type="math/tex; mode=display">\max{D(x)}</script><script type="math/tex; mode=display">w^Tw = 1</script></li>
<li><p>通过求解和线性代数可发现，数据集投影后的方差就是 <strong>协方差矩阵 </strong> 的 <strong> 特征值 </strong>，<strong> 投影的方向 </strong> 就是 特征值对应的 <strong> 特征向量</strong>。</p>
</li>
<li><p>PCA 求解方法：</p>
<ul>
<li><ol>
<li>对样本进行中心化处理</li>
</ol>
</li>
<li><ol>
<li>求样本协方差矩阵</li>
</ol>
</li>
<li><ol>
<li>对协方差矩阵进行特征值分解，并从大到小排序</li>
</ol>
</li>
<li><ol>
<li>取特征值前 d 的大的特征向量 $w_1, …, w_d$。 通过映射还原 n 维度到 d 维度<script type="math/tex; mode=display">x_i^’ = [w_1^Tx_i; ..., w_d^Tx_i]</script></li>
</ol>
</li>
</ul>
</li>
<li><p>PCA 降维后的信息占比为： $\eta = \sqrt{\frac{\sum<em>{i=1}^d{\lambda_i^2}}{\sum</em>{i=1}^n{\lambda_i^2}}}$</p>
</li>
</ul>
<h4 id="4- 线性判别模型（LDA-linear-discriminant-analysis"><a href="#4- 线性判别模型（LDA-linear-discriminant-analysis" class="headerlink" title="4. 线性判别模型（LDA, linear discriminant analysis)"></a>4. 线性判别模型（LDA, linear discriminant analysis)</h4><ul>
<li><p>相比于 PCA, LDA 可以作为一种基于标签的有监督的降维方法（如下图具有不同标签，PCA 将映射到 y 轴，相反 LDA 将映射到 x 轴，后者更好）。<br><img src="/imgs/bmjqxx/4-4.png" style="width:80%;height:80%;text-align:center"></p>
</li>
<li><p>核心思想：最大化类间间距、最小化类内间距。（同样它对数据的分布做了一些很强的假设。认为每个类数据都是高斯分布，且各个类的协方差相等）。</p>
</li>
<li><p>公式推理：以两个类为例子 $C_1, C_2$。</p>
<ul>
<li><p>最大化类内间距：需要找到一个投影平面 $w$, 最大化投影后两个类中心之间的距离。  </p>
<script type="math/tex; mode=display">\mu_1 = \frac{1}{N_1}\sum_{x\in C_1}x, \mu_2 = \frac{1}{N_2}\sum_{x\in C_2}x</script><script type="math/tex; mode=display">
\mathop\limits^\sim=w^T\mu_1, \mathop\limits^\sim=w^T\mu_2</script><script type="math/tex; mode=display">D(C_1, C_2) = ||\mathop\limits^\sim - \mathop\limits^\sim||^2</script><script type="math/tex; mode=display">\max_w||w^T(\mu_1 - \mu_2)||_2^2
    w^Tw = 1</script><p>当 $w$ 与 $(\mu_1 - \mu_2)$ 方向一致的时候，该距离最大。此时可能出现如下图问题，就是投影后两个类出现了重叠。<br><img src="/imgs/bmjqxx/4-5a.png" style="width:80%;height:80%;text-align:center"></p>
</li>
<li><p>最小化类间间距：缩小类间距离，如下图所示，能够改善上图中的问题。<br><img src="/imgs/bmjqxx/4-5b.png" style="width:80%;height:80%;text-align:center"><br>目标函数: $D_1, D_2$ 表示投影后的类间距离</p>
<script type="math/tex; mode=display">max_wJ(w) = \frac{||w^T(\mu_1 - \mu_2)||_2^2}{D_1 + D_2}</script><script type="math/tex; mode=display">
 D_1 = \sum_{x\in C_1}(w^Tx - w^T\mu_1)^2</script><script type="math/tex; mode=display">D_2 = \sum_{x\in C_2}(w^Tx - w^T\mu_2)^2</script><script type="math/tex; mode=display">J(w) = \frac{w^T(\mu_1-\mu_2)(\mu_1-\mu_2)^Tw}{\sum_{x \in C_i}w^T(x-\mu_i)(x-\mu_i)^Tw}</script><script type="math/tex; mode=display">类间散度矩阵： S_B = (\mu_1-\mu_2)(\mu_1-\mu_2)^T</script><script type="math/tex; mode=display">类内散度矩阵： S_W = \sum_{x \in C_i}(x-\mu_i)(x-\mu_i)^T</script><p>对目标函数求导并令倒数为 0 得到。</p>
<script type="math/tex; mode=display">S_w^{-1}S_Bw = \alpha w</script></li>
</ul>
</li>
</ul>
<h4 id="5- 简述 -K- 均值算法的具体步骤"><a href="#5- 简述 -K- 均值算法的具体步骤" class="headerlink" title="5. 简述 K 均值算法的具体步骤"></a>5. 简述 K 均值算法的具体步骤</h4><p>K 均值聚类的核心目标是将给定的数据集分类为 K 个族，并给出每个数据对应的簇中心点。其步骤如下：</p>
<ul>
<li><ol>
<li>数据预处理，如归一化、离群点处理等。</li>
</ol>
</li>
<li><ol>
<li>随机选取 K 个簇中心，记为 $u_1^{(0)}, …, u_k^{(0)}$。</li>
</ol>
</li>
<li><ol>
<li>定义代价函数: $J(c, u) = \min<em>u\min_c\sum</em>{i=1}^M||x<em>i - u</em>{c_i}||^2$</li>
</ol>
</li>
<li><ol>
<li>令 $t = 0, 1, …$ 为迭代部署，重复下面过程直到 $J$ 收敛。</li>
</ol>
</li>
</ul>
<h4 id="6-K- 均值算法的优缺点？如何对其调优？"><a href="#6-K- 均值算法的优缺点？如何对其调优？" class="headerlink" title="6. K 均值算法的优缺点？如何对其调优？"></a>6. K 均值算法的优缺点？如何对其调优？</h4><ul>
<li><p>优点：对于大数据集，K 均值算法是可伸缩和高效的，它的计算复杂度是 O(NKt)线性的，$N, K, t$ 分别表示样本数目，聚类簇数和迭代的轮数。</p>
</li>
<li><p>缺点：受初值和离群点的影响，每次结果不稳定，且结果通常是局部最优解不是全局最优解。</p>
</li>
<li><p>调优方法：</p>
<ul>
<li><ol>
<li>数据归一化和离群点处理： 其距离计算公式基于欧式距离度量，均值和方差大的维度对数据聚类结果会产生决定性影响。未做归一化和统一单位的数据会无法直接参与运算和比较。离群点或噪声数据会导致中心偏移。</li>
</ol>
</li>
<li><ol>
<li>合理 K 值：基于经验和多次试验结果。尝试不同的 K 值，然后寻找折弯处。</li>
</ol>
</li>
<li><ol>
<li>采用核方法。</li>
</ol>
</li>
</ul>
</li>
</ul>
<h4 id="7-K- 均值算法的缺点，有哪些改进模型？"><a href="#7-K- 均值算法的缺点，有哪些改进模型？" class="headerlink" title="7. K 均值算法的缺点，有哪些改进模型？"></a>7. K 均值算法的缺点，有哪些改进模型？</h4><ul>
<li><p>主要缺点:</p>
<ul>
<li><ol>
<li>需要人工预先确定初始 K 值，该值和真实的数据分布未必吻合。</li>
</ol>
</li>
<li><ol>
<li>K 均值只能收敛到局部最优，效果受到初值影响很大。</li>
</ol>
</li>
<li><ol>
<li>容易受到噪声点的影响</li>
</ol>
</li>
<li><ol>
<li>样本只能被划分到单一的类中</li>
</ol>
</li>
</ul>
</li>
<li><p>改进模型：</p>
<ul>
<li><ol>
<li>K-means++：在初始类中心选取的时候，聚类中心会相互离的很远，不再是随机选择。</li>
</ol>
</li>
<li><ol>
<li>ISODATA: K 值的选取。核心思想：当某个类别的样本数过少时，把该类去掉；当某个类的样本数过多、分数程度大的时候，将该类分为两个子类别。<strong>此时对应了三个参数，预期的聚类中心数目，每个类要求的最少样本数目（用于去掉某个类别），最大方差（用于控制某个类别中样本的分散程度），两个聚类中心之间允许的最小距离（当两个类靠很近时，进行合并操作）</strong> </li>
</ol>
</li>
</ul>
</li>
</ul>
<h4 id="8- 高斯混合模型的核心思想？它是如何迭代计算的？"><a href="#8- 高斯混合模型的核心思想？它是如何迭代计算的？" class="headerlink" title="8. 高斯混合模型的核心思想？它是如何迭代计算的？"></a>8. 高斯混合模型的核心思想？它是如何迭代计算的？</h4><ul>
<li><p>高斯混合模型是 <strong>生成式模型</strong>。核心思想假设数据可以看做从多个高斯分布中生成出来。在该假设下，每个单独的分模型都是标准高斯模型，其均值 和 方差 $\mu_i, \theta_i$ 都是待估计参数。每个分模型还有一个参数 $\pi_i$， 理解为权重和生成数据的概率。</p>
<script type="math/tex; mode=display">p(x) = \sum_{i=1}^KN(x | \mu_i, \theta_i)</script></li>
<li><p>高斯混合模型的求解为 <strong>EM 算法</strong></p>
<ul>
<li>E-step: 根据当前参数，计算某个点由某个分模型生成的概率</li>
<li>M-step: 根据 E 步估计的概率，通过求导令偏导数为 0，改进每个分模型的均值、方差和权重参数。</li>
</ul>
</li>
</ul>
<h4 id="以聚类问题为例，假设没有外部标签，如何评估两个聚类算法的优劣？"><a href="# 以聚类问题为例，假设没有外部标签，如何评估两个聚类算法的优劣？" class="headerlink" title="以聚类问题为例，假设没有外部标签，如何评估两个聚类算法的优劣？"></a>以聚类问题为例，假设没有外部标签，如何评估两个聚类算法的优劣？</h4><ul>
<li><p>以中心定义的数据簇：聚类后的集合到其中心（该类的评价值）的距离越小越好</p>
</li>
<li><p>以密度定义的数据粗：这类数据城乡与周围数据簇明显不同的密度。该情况通常用于没有噪声点和离群点。</p>
</li>
<li><p>以连通定义的数据簇： 这里数据集合中数据点和数据点之间有连接关系，整个数据簇表现为图结构。该定义对不规则形状和缠绕的数据簇有效。</p>
</li>
<li><p>以概念定义的数据簇：该类数据集合中所有数据点具有某种性质。</p>
</li>
<li><p>聚类算法的评估：</p>
<ul>
<li><p>CH 指标 (Calinski-Harabasz)：</p>
<script type="math/tex; mode=display">CH(K) = \frac{tr(B) / (K - 1)}{tr(W) / (N - K)}</script><script type="math/tex; mode=display">tr(B) = \sum_{j=i}^k||z_j - z||^2</script><script type="math/tex; mode=display">tr(W) = \sum_{j=1}^k\sum_{x_{i} \in z_k} ||x_i - z_j||^2</script></li>
<li><p>轮廓系数 (Silhouette Coefficient):<br>每个样本都对应轮廓系数，其由两部分组成：$a$: 样本与同一簇中其他样本的平均距离。 $b$: 样本距离最近簇类中所有样本点的平均距离。<br>轮廓系数：</p>
<script type="math/tex; mode=display">
s = \frac{b - a}{max(a, b)}</script></li>
</ul>
</li>
</ul>
<h4 id="常见的概率图模型中，那些是生成式模型？那些是判别式模型？"><a href="# 常见的概率图模型中，那些是生成式模型？那些是判别式模型？" class="headerlink" title="常见的概率图模型中，那些是生成式模型？那些是判别式模型？"></a>常见的概率图模型中，那些是生成式模型？那些是判别式模型？</h4><ul>
<li><p>判别式模型与生成式模型的区别：假设课管泽的变量集合为 $X$， 需要预测的变量集合为 $Y$, 其他变量集合为 $Z$。 生成式模型对联合概率 $P(X, Y, Z)$ 进行建模。在给定观测集合 $X$ 的条件下，通过计算边缘分布来得到对变量集合 $Y$ 的推断。即</p>
<script type="math/tex; mode=display">P(Y | X) = \frac{P(X, Y)}{P(X)} = \frac{\sum_ZP(X, Y, Z)}{\sum_{Y, Z}P(X, Y, Z)}</script><p>判别式模型则是直接对条件概率分布 $P(Y, Z|X)$ 进行建模，消去无关变量 $Z$ 就可以得到对变量集合 $Y$ 的预测。</p>
<script type="math/tex; mode=display">P(Y | X) = \sum_ZP(Y, Z | X)</script></li>
<li><p>生成式模型：朴素贝叶斯，贝叶斯网络，pLSA, LDA, HMM</p>
</li>
<li><p>判别式模型：最大熵模型, CRF</p>
</li>
<li><p>HMM —&gt; MEMM(最大熵马尔科夫模型)(解决 HMM 输出独立性假设的问题，但是只是解决了观测值独立的问题，状态之间的假设可能会产生标注偏置问题) —&gt; CRF(解决了标注偏置问题，去除了 HMM 中两个不合理的假设)</p>
</li>
</ul>
]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>白板推导 - 机器学习 (数学基础)</title>
    <url>/2021/04/12/%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h4 id="1- 频率派 -vs- 贝叶斯派"><a href="#1- 频率派 -vs- 贝叶斯派" class="headerlink" title="1. 频率派 vs 贝叶斯派"></a>1. 频率派 vs 贝叶斯派 </h4><p> 数据： $X = (x_1, x_2, …, x_n)^T_{N \times p}$  参数:  $\theta$</p>
<ul>
<li><p>频率派: $\theta$ 是未知参数，$X$ 是随机变量(r.v.)</p>
<p>MLE: $\theta = \argmax_{\theta} P(X | \theta)$</p>
<p>引申为统计机器学习，构造 loss function. 进行梯度下降法等参数优化训练</p>
</li>
<li><p>贝叶斯派：$\theta$ 为 r.v., 且具有先验分布 $\theta \sim P(\theta)$</p>
<p>MAP: $\theta = \argmax_{\theta} P(\theta | X) = \argmax_{\theta} P(X|\theta)\cdot P(\theta)$</p>
<p>贝叶斯估计：$P(\theta | X) = \frac{P(X | \theta)P(\theta)}{\int_{\theta}P(X|\theta)P(\theta) d\theta}$</p>
<p>贝叶斯预测：给定观测 $\hat{x}$, $P(\hat{x}|X) = \int_{\theta}P(\hat{x},\theta|X) = \int_{\theta}P(\hat{x}|\theta)P(\theta|X) d\theta$</p>
<p>引申为概率图模型，求积分， MCMC(蒙特卡洛采样)</p>
</li>
</ul>
<span id="more"></span>

<h4 id="2- 高斯分布 - 极大似然估计"><a href="#2- 高斯分布 - 极大似然估计" class="headerlink" title="2. 高斯分布 - 极大似然估计"></a>2. 高斯分布 - 极大似然估计 </h4><p> 数据：$X = (x_1, …, x_n)^T_{N \times p}， x_i \in {R^p}$ $x_i \sim N(\mu, \xi)$ 其样本之间独立同分布，$\theta = (\mu, \xi)$。</p>
<p> 假设 $p = 1$, $\theta = (\mu, \delta^2)$</p>
<p>一维高斯分布： $P(x|\theta) = \frac{exp(-\frac{(x-\mu)^2}{2\delta^2})}{\sqrt{2\pi}\delta}$</p>
<p>p 维高斯分布：$P(x|\theta) = \frac{exp(-\frac{1}{2}(x-\mu)^T\xi^{-1}(x-\mu)}{(2\pi)^{\frac{p}{2}}|\xi|^{\frac{1}{2}}}$</p>
<ul>
<li><p>公式推导：<br>MLE: $$\theta = \argmax_\theta P(X | \theta)$$<br>$$logP(X|\theta) = log\prod P(x_i|\theta) = \sum_{i=1}^N log P(x_i|\theta) $$<br>$$ = \sum_{i=1}^N log \frac{1}{\sqrt{2\pi}\delta}exp(-\frac{(x_i-\mu)^2}{2\delta^2})$$<br>$$ = \sum_{i=1}^N log\frac{1}{\sqrt{2\pi}}+log\frac{1}{\delta}-\frac{(x_i-\mu)^2}{2\delta^2}$$</p>
<p>极大似然估计求导，得到参数估计：<br>$${\mu}<em>{mle} = \argmax</em>{\mu} P(X|\theta)$$<br>$$= \argmax_{\mu} \sum_{i=1}^N - \frac{(x_i - \mu)^2}{2\delta^2}$$<br>$$= \argmin_{\mu} \sum_{i=1}^N {(x_i - \mu)^2}$$</p>
<p>$$\frac{\partial\sum(x-\mu)^2}{\partial\mu}=0 \to \mu_{mle} = \frac{1}{N}\sum x_i$$</p>
<p>$$\frac{\partial\sum(-log\delta-\frac{1}{2\delta^2}(x_i-\mu)^2)}{\partial\delta} = \sum_{i=1}^N[-\frac{1}{\delta}+(x_i-\mu)^2\delta^{-3}] = 0$$<br>$$\delta_{mle}^2 = \frac{1}{N}\sum_{i=1}^N(x_i - \mu_{mle})^2$$</p>
</li>
</ul>
<h4 id="3- 高斯分布 - 极大似然估计（有偏 -vs- 无偏）"><a href="#3- 高斯分布 - 极大似然估计（有偏 -vs- 无偏）" class="headerlink" title="3. 高斯分布 - 极大似然估计（有偏 vs 无偏）"></a>3. 高斯分布 - 极大似然估计（有偏 vs 无偏）</h4><p>无偏的定义： 对于给定参数  $\theta$, 从实际数据得到的估计值 $\hat{\theta}$, 如何 $E(\hat{\theta}) = \theta$, 那么就是无偏的，否则有偏。</p>
<ul>
<li><p>$E(\mu_{mle}) = E[\frac{1}{N}\sum_{i=1}^N x_i] = \frac{1}{N}\sum_{i=1}^N E[x_i] = \mu$  无偏</p>
</li>
<li><p>$$E[\delta_{mle}^2] = E[\frac{1}{N}\sum_{i=1}^N(x_i-\mu_{mle})^2]$$<br>$$ = E[\frac{1}{N}\sum_{i=1}^N(x_i^2 - 2x_i\mu_{mle} + \mu_{mle}^2)]$$<br>$$ = E[\frac{1}{N}\sum_{i=1}^N (x_i^2 - \mu_{mle}^2)]$$<br>$$ = E[\frac{1}{N}\sum_{i=1}^N(x_i^2-\mu^2)] + E[\frac{1}{N}\sum_{i=1}^N(\mu_{mle}-\mu^2)]$$<br>$$ = E[\frac{1}{N}\sum_{i=1}^N\delta^2] - Var(\mu_{mle}) {即为 \frac{1}{N}\delta^2} = \frac{N-1}{N}\delta^2$$<br>所以 $\delta_{mle}^2$ 是有偏的。</p>
</li>
</ul>
<h4 id="4- 高斯分布 - 从概率密度角度观察"><a href="#4- 高斯分布 - 从概率密度角度观察" class="headerlink" title="4. 高斯分布 - 从概率密度角度观察"></a>4. 高斯分布 - 从概率密度角度观察</h4><ul>
<li><p>$X \sim N(\mu, \Sigma), P(x) = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}} exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$, $\Sigma$ 是正定的</p>
</li>
<li><p>$(x-\mu)^T\Sigma^{-1}(x-\mu)$： 表示马氏距离，其中当 $\Sigma = I$ 是，马氏距离即为欧氏距离。</p>
</li>
<li><p>$\Sigma = U\wedge U^T$ 其中 $UU^T=U^TU=I$, $\wedge = diag(\lambda_i)$, $U=(u_1,…,u_p)_{p\times p}$</p>
</li>
<li><p>$\Delta = (x-\mu)^T\Sigma^{-1}(x-\mu) = \sum_{i=1}^p\frac{y_i^2}{\lambda_i^2}$, 其中 $y_i = (x-\mu)^Tu_i$</p>
</li>
</ul>
<h4 id="5- 高斯分布 - 局限性"><a href="#5- 高斯分布 - 局限性" class="headerlink" title="5. 高斯分布 - 局限性"></a>5. 高斯分布 - 局限性</h4><ul>
<li><p>$\Sigma_{p\times p} \to \frac{p^2 - p}{2} \to 参数个数 O(p^2)$</p>
</li>
<li><p>参数个数，改进为假设 $Sigma = diag(\lambda_1, …, \lambda_p)$ 为对角矩阵</p>
</li>
<li><p>参数问题，假设模型为多个高斯模型的混合 GMM.</p>
</li>
</ul>
<h4 id="6- 高斯分布 - 求边缘概率和条件概率"><a href="#6- 高斯分布 - 求边缘概率和条件概率" class="headerlink" title="6. 高斯分布 - 求边缘概率和条件概率"></a>6. 高斯分布 - 求边缘概率和条件概率 </h4><h4 id="7- 线性回归 - 最小二乘法及其几何意义"><a href="#7- 线性回归 - 最小二乘法及其几何意义" class="headerlink" title="7. 线性回归 - 最小二乘法及其几何意义"></a>7. 线性回归 - 最小二乘法及其几何意义</h4><p> 给定数据集合 $D = {(x_1, y_1), (x_2, y_2), …, (x_N, y_N))}$, $x_i \in R^P$, $y_i \in R$<br>$X = (x_1, x_2, …, x_N)^T, X \in R^{N \times p}$</p>
<ul>
<li>公式： $$L(w) = \sum_{i=1}^N ||w^T x_i - y_i||^2$$<pre><code>   $$=\sum_&#123;i=1&#125;^N (w^T x_i - y_i)^2$$
   $$=(W^T X^T - Y^T)(XW - Y)$$
   $$=W^TX^TXW - 2W^TX^TY + Y^TT$$
   $$=W^TX^TXW - 2W^TX^TY$$
</code></pre>
$\hat{W} = \argmin{L(W)} \to \frac{\partial{L(W)}}{\partial{W}} = 2X^TXW - 2X^TY = 0$ 即 $W = (X^TX)^{-1}X^TY$</li>
</ul>
<h4 id="8- 线性回归 - 概率视角 -MLE- 高斯噪声"><a href="#8- 线性回归 - 概率视角 -MLE- 高斯噪声" class="headerlink" title="8. 线性回归 - 概率视角 -MLE- 高斯噪声"></a>8. 线性回归 - 概率视角 -MLE- 高斯噪声 </h4><p> 假设模型拟合存在噪声 $\varepsilon \sim N(0, \delta^2)$, 样本之间独立同分布。<br>$y = f(w) + \varepsilon = w^Tx + \varepsilon$</p>
<p>则 $y|x,w \sim N(W^Tx, \delta^2)$<br>$$P(y|x; w) = \frac{1}{\sqrt{2\pi}\delta}exp(-\frac{(y-w^Tx)^2}{2\delta^2})$$</p>
<ul>
<li><p>MLE:<br>$$L(w) = logP(Y|X; w) = \sum_{i=1}^NlogP(y_i|x_i; w)$$<br>$$ = \sum_{i=1}^N log\frac{1}{\sqrt{2\pi}\delta} - \frac{1}{2\delta^2}(y_i-w^Tx_i)^2$$</p>
<p>即 $$\hat{w} = \argmax_w L(w)$$<br>   $$= \argmax_w - \frac{1}{2\delta^2}(y_i-w^Tx_i)^2$$<br>   $$= \argmin_w (y_i-w^Tx_i)^2$$ </p>
<p>即得到最小二乘的公式</p>
</li>
</ul>
<h4 id="9- 线性回归 - 正则化 - 岭回归 - 频率角度"><a href="#9- 线性回归 - 正则化 - 岭回归 - 频率角度" class="headerlink" title="9. 线性回归 - 正则化 - 岭回归 - 频率角度"></a>9. 线性回归 - 正则化 - 岭回归 - 频率角度 </h4><p> 正则化框架:  $\argmin_w L(w) + \lambda P(w)$</p>
<ul>
<li><p>L1: Lasso,  $P(w) = ||w||_1$</p>
</li>
<li><p>L2: Ridge, 岭回归， $P(w) = ||w||_2^2 = w^Tw$。 权值衰减</p>
</li>
</ul>
<p>加上 L2 正则的损失函数：<br>$$J(W) = \sum_{i=1}^N ||W^Tx_i - y_i||^2 + \lambda W^TW$$<br>$$ = (W^TX^T - Y^T)(XW - Y) + \lambda W^TW$$<br>$$ = W^T (X^TX + \lambda I) W - 2W^TX^TY + Y^TY$$<br>求导等于 0 $\hat{W} = \argmin J(W) \to \hat{W} = (X^TX+\lambda I)^{-1}X^TY$</p>
<h4 id="10- 线性回归 - 正则化 - 岭回归 - 贝叶斯角度"><a href="#10- 线性回归 - 正则化 - 岭回归 - 贝叶斯角度" class="headerlink" title="10. 线性回归 - 正则化 - 岭回归 - 贝叶斯角度"></a>10. 线性回归 - 正则化 - 岭回归 - 贝叶斯角度</h4><ul>
<li><p>LSE(最小二乘估计) = MLE (噪声为高斯分布的 极大似然估计)</p>
</li>
<li><p>Regulariezed LSE (加入正则化的最小二乘估计） = MAP （噪声和先验都是高斯分布的最大后验估计）</p>
</li>
</ul>
<p>噪声为高斯分布的线性回归：<br>$$f(w) = w^Tx$$<br>$$y = f(w) + \varepsilon = w^Tx+\varepsilon$$<br>$$\varepsilon \sim N(0, \delta^2)$$<br>$$y|x;w \sim N(w^Tx, \delta^2)$$<br>$$P(y|x;w) = \frac{1}{\sqrt{2\pi}\delta}exp(-\frac{(y-w^Tx)^2}{2\delta^2})$$</p>
<p>贝叶斯角度：假设先验参数也是服从高斯分布   $w \sim N(0, \delta_0^2)$<br>$$P(w|y) = \frac{P(y|w)P(w)}{P(y)}$$</p>
<p>MAP: $$\hat{w} = \argmax_w P(w|y)$$<br>     $$= \argmax P(y|w)P(w)$$<br>     $$= \argmax \sum_{i=1}^N log[P(y|w)P(w)]$$<br>     $$= \argmax \sum_{i=1}^N log\frac{1}{\sqrt{2\pi}\delta \sqrt{2\pi}\delta_0^2} + log exp(-\frac{(y-w^Tx)^2}{2\delta^2} - \frac{||w||^2}{2\delta_0^2})$$<br>     $$= \argmin_w(\frac{(y-w^Tx)^2}{2\delta^2} + \frac{||w||^2}{2\delta_0^2})$$<br>     $$= \argmin_w((y-w^Tx)^2 + \frac{\delta^2}{\delta_0^2}||w||^2)$$<br>即为 带正则化的最小二乘估计。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>视频笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>白板推导 - 机器学习 - 线性分类</title>
    <url>/2021/04/20/%E7%99%BD%E6%9D%BF%E6%8E%A8%E5%AF%BC-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/</url>
    <content><![CDATA[<h4 id="1- 线性分类 - 背景"><a href="#1- 线性分类 - 背景" class="headerlink" title="1. 线性分类 - 背景"></a>1. 线性分类 - 背景</h4><p>Linear Regression: $f(w, b) = w^T x + b$</p>
<ul>
<li><p>线性：</p>
<ul>
<li>属性非线性： 特征转换（多项式回归）</li>
<li>全局非线性： 线性分类（激活函数是非线性）</li>
<li>系数非线性： 神经网络，感知机</li>
</ul>
</li>
<li><p>全局性：线性样条回归，决策树</p>
</li>
<li><p>数据未加工： PCA,  流形</p>
</li>
</ul>
<p>分类：</p>
<ul>
<li><p>硬分类： 0 或者 1</p>
<ul>
<li>线性判别模型 fisher:</li>
<li>感知机</li>
</ul>
</li>
<li><p>软分类： 0 到 1 的概率值</p>
<ul>
<li>生成式：高斯判别模型</li>
<li>判别式：逻辑回归</li>
</ul>
</li>
</ul>
<span id="more"></span>
<h4 id="2- 线性分类 - 感知机"><a href="#2- 线性分类 - 感知机" class="headerlink" title="2. 线性分类 - 感知机"></a>2. 线性分类 - 感知机 </h4><p> 思想： 错误驱动</p>
<p>模型： $f(x) = sign(w^Tx), x\in R^p, w\in R^p$,<br>当 a&gt;=0, sign(a) = 1, 否则, sign(a) = -1</p>
<p>策略：被错误分类的点的个数， loss function<br>$$L(w) = \sum_{i=1}^N I(y_iw^Tx_i &lt; 0) \to L(w) = \sum_{i=1}^N -y_iw^Tx_i$$ </p>
<p>求解方法：SGD: $w^{t+1}=w^t - \lambda\frac{\partial L(w)}{\partial w} = w^t+\lambda y_ix_i$</p>
<h4 id="3- 线性分类 - 线性判别分析（Fisher"><a href="#3- 线性分类 - 线性判别分析（Fisher" class="headerlink" title="3. 线性分类 - 线性判别分析（Fisher)"></a>3. 线性分类 - 线性判别分析（Fisher)</h4><ul>
<li><p>假设符号表示： $X = (x_1, …, x_N)^T \in R^{N\times p}, x_i\in R^p, y_i\in(+1, -1)$,<br>  $X_{c_1} = {x_i | y_i = +1}, X_{c_2} = {x_i | y_i = -1}$, $|X_{c_1}|=N_1, X_{c_2}=N_2, N_1+N_2 = N$</p>
</li>
<li><p>思想： 类内小， 类间大。 </p>
</li>
<li><p>公式<br>$$z_i = w^T x_i$$<br>$$\hat{z} = \frac{1}{N}\sum_{i=1}^Nz_i = \frac{1}{N}w^Tx_i$$<br>$$S_z = \frac{1}{N}\sum_{i=1}^N (z_i - \hat{z})(z_i - \hat{z})^T$$<br>$$ = \frac{1}{N}\sum_{i=1}^N(w^Tx_i - \hat{z})(w^Tx_i - \hat{z})^T$$</p>
<p>$$c_1: \hat{z_1} = \frac{1}{N_1}\sum_{i=1}^{N_1}w^Tx_i$$<br>$$S_1 = \frac{1}{N_1}\sum_{i=1}^{N_1}(w^Tx_i - \hat{z_1})(w^Tx_i - \hat{z_1})^T$$<br>$$c_2: \hat{z_2} = \frac{1}{N_2}\sum_{i=1}^{N_2}w^Tx_i$$<br>$$S_2 = \frac{1}{N_2}\sum_{i=1}^{N_2}(w^Tx_i - \hat{z_2})(w^Tx_i - \hat{z_2})^T$$</p>
<p>类间：$(\hat{z_1} - \hat{z_2})^2$ </p>
<p>类内：$S_1 + S_2$</p>
<p>目标函数：<br>$$J(w) = \frac{(\hat{z_1} - \hat{z_2})^2}{S_1 + S_2} = \frac{w^T(\overline{x_{c_1}} - \overline{x_{c_2}})w}{w^T(S_{c_1} + S_{c_2})w}$$<br>$$ = \frac{w^T S_b w}{w^T S_w w}$$</p>
<p>$S_b: between-class 类间方差 $</p>
<p>$S_w: with-class 类内方差 $</p>
<p>最终化简得到： $w \propto (\overline{x_{c_1}} - \overline{x_{c_2}})$</p>
</li>
</ul>
<h4 id="4- 线性分析 - 逻辑回归"><a href="#4- 线性分析 - 逻辑回归" class="headerlink" title="4. 线性分析 - 逻辑回归"></a>4. 线性分析 - 逻辑回归</h4><ul>
<li><p>Data: ${(x_i, y_i)}_{i-1}^N, x_i \in R^p, y_i\in{0, 1}$</p>
</li>
<li><p>sigmoid function: $\delta(z) = \frac{1}{1+exp(-z)}$<br>$$P(y=1|x) = \delta(w^Tx) = \frac{1}{1+exp(-w^T x)}, y=1$$<br>$$P(y=0|x) = 1 - P(y=1|x) = 1- \delta(w^Tx) = \frac{exp(-w^T x)}{1+exp(-w^T x)}, y=0$$<br>$$P(y|x) = P(y=1|x)^y P(y=0|x)^{1-y}$$</p>
</li>
<li><p>MLE 得到 交叉熵函数(cross entropy):<br>$$\hat{w} = \argmax_w \prod_{i=1}^N p(y_i|x_i)$$<br>$$= \argmax_w \sum_{i=1}^N y_ilog P(y=1|x)+(1-y_i)log P(y=0|x)$$<br>$$= \argmin_w  -\sum_{i=1}^N y_ilog P(y=1|x)+(1-y_i)log P(y=0|x)$$</p>
</li>
</ul>
<h4 id="5- 线性分析 - 高斯判别分析"><a href="#5- 线性分析 - 高斯判别分析" class="headerlink" title="5. 线性分析 - 高斯判别分析"></a>5. 线性分析 - 高斯判别分析</h4><ul>
<li><p>Data: ${(x_i, y_i)}_{i-1}^N, x_i \in R^p, y_i\in{0, 1}$</p>
</li>
<li><p>生成式模型：<br>$$\hat{y}= \argmax P(y|x) = \argmax_y P(y)P(x|y)$$<br>$$y \sim Bernoulli(\phi) \to {P(y=1)=\phi 且 P(y=0)=1-\phi }$$<br>$$x | y=1 \sim N(\mu_1, \xi)$$<br>$$x | y=0 \sim N(\mu_2, \xi)$$</p>
</li>
<li><p>log-likelihood:<br>$$l(\theta) = log\prod_{i=1}^N P(x_i, y_i)$$<br>$$ = \sum_{i=1}^Nlog (P(x_i|y_i)P(y_i))$$<br>$$ = \sum_{i=1}^N[log P(x_i|y_i) + log P(y_i)]$$<br>$$ = \sum_{i=1}^N[log N(\mu_1, \xi)^{y_i} N(\mu_2, \xi)^{1-y_i}+ log \phi^{y_i} (1-\phi)^{1-y_i}]$$<br>$$ = \sum_{i=1}^N[log N(\mu_1, \xi)^{y_i} + \log N(\mu_2, \xi)^{1-y_i}+ log \phi^{y_i} + log(1-\phi)^{1-y_i}]$$</p>
</li>
<li><p>假设：<br>$$\hat{\theta} = \argmax_{\theta} l(\theta)$$<br>$$\theta = (\mu_1, \mu_2, \xi, \phi)$$<br>$$y=1: N_1$$<br>$$y=0: N_2$$<br>$$N = N_1 + N_2$$</p>
</li>
<li><p>求偏导：<br>$$\frac{\partial l(\theta)}{\partial \phi} = \sum_{i=1}^N\frac{y_i}{\phi} + \frac{1-y_i}{1-\phi} = 0$$<br>$$\to \sum_{i=1}^N y_i(1-\phi) - (1-y_i)\phi = \sum_{i=1}^N (y_i - \phi) = 0$$<br>$$\to \sum_{i=1}^Ny_i - N\phi = 0$$<br>$$\to \hat{\phi} = \sum_{i=1}^N = \frac{N_1}{N}$$</p>
</li>
</ul>
<p>  $$① = \sum_{i=1}^N logN(\mu_1,\xi)^{y_i} = \sum_{i=1}^Ny_i log \frac{1}{(2\pi)^\frac{p}{2}|\xi|^\frac{1}{2}}exp(-\frac{1}{2}(x_i - \mu_1)^T\xi^{-1}(x_i - \mu_1))$$<br>  $$\mu_1 = \argmax_{\mu_1}① = \argmax\sum_{i=1}^N y_i(-\frac{1}{2}(x_i - \mu_1)^T\xi^{-1}(x_i - \mu_1))$$<br>  $$\frac{\partial ①}{\partial \mu_1} = -\frac{1}{2}\sum_{i=1}^Ny_i(-2\xi^{-1}x_i + 2\xi^{-1}\mu_1) = 0$$<br>  $$\to \sum_{i=1}^N y_i(\mu_1 - x_i) = 0$$<br>  $$\to \hat{\mu_1} = \frac{\sum_{i=1}^Ny_ix_i}{\sum_{i=1}y_i} =\frac{\sum_{i=1}^Ny_ix_i}{N_1}$$</p>
<ul>
<li>同样的 MLE 求最大两个参数。</li>
</ul>
<h4 id="6- 线性分类 - 朴素贝叶斯分类器"><a href="#6- 线性分类 - 朴素贝叶斯分类器" class="headerlink" title="6. 线性分类 - 朴素贝叶斯分类器"></a>6. 线性分类 - 朴素贝叶斯分类器</h4><ul>
<li><p>条件独立性假设: $P(x|y) = \prod_{i=1}^N P(x_i|y)$</p>
</li>
<li><p>目的（动机）： 简化运算。</p>
</li>
<li><p>$\hat{y} = \argmax_y P(y|x) = \argmax_y \frac{P(x, y)}{P(x)} = \argmax_y P(y) P(x|y)$</p>
</li>
<li><p>x 的分类</p>
<ul>
<li>x 是离散的, $x_j \sim Categorical Dist$</li>
<li>x 是连续的, $x_j \sim N(\mu_j, \eth^2)$</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>视频笔记</tag>
      </tags>
  </entry>
</search>
